{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import os\n",
    "os.chdir('H:/Mes Documents/ServiceCivil2019/schedvisu')\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from main import run, load_config, get_day_range\n",
    "from retrieve_data import *\n",
    "from extract_data import load_transform_and_save_data_from_files\n",
    "from create_report import create_report, get_report_type\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set the width of the notebook\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for creating a multi-page PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Dose report #5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series = pd.read_pickle('C:/TEMP/SchedVisu_data/series.pkl')\n",
    "print(df_series.columns)\n",
    "df_ctdivol = df_series[(~df_series['CTDIvol'].isnull()) | (~df_series['CTDIvol_start'].isnull())][['Date', 'CTDIvol_start', 'Series Description', 'Study Description', 'Protocol Name', 'Modality', 'Patient ID', 'ImageType']]\n",
    "display(set(df_ctdivol['Series Description']))\n",
    "display(set(df_ctdivol['Protocol Name']))\n",
    "display(set(df_ctdivol['Study Description']))\n",
    "display(df_ctdivol[(~df_ctdivol['Series Description'].str.match('.*low.*')) & (~df_ctdivol['Series Description'].str.match('.*Topogram.*'))])\n",
    "\n",
    "df_studies = pd.read_pickle('C:/TEMP/SchedVisu_data/studies.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Fetch more fields from the DICOMs #38\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "day = dt(2019, 2, 27)\n",
    "day_str = day.strftime('%Y%m%d')\n",
    "patientID = ['2026682', '138821']\n",
    "\n",
    "config = load_config()\n",
    "config['main']['start_date'] = day.strftime('%Y%m%d')\n",
    "config['main']['end_date'] = day.strftime('%Y%m%d')\n",
    "\n",
    "df_studies = find_studies_for_day(config, day_str, ['PT'])\n",
    "df_studies = df_studies.query('`Patient ID` in @patientID')\n",
    "\n",
    "df_series = find_series_for_studies(config, df_studies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Manually fetch the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_save = df_series.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_series, df_series_failed = fetch_info_for_series_with_batches(config, df_series_save.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for retrieving all data for calculating correct End Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the fetching so that it keeps all info for calculating end times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_and_save_single_day_data_from_PACS(config, datetime.datetime.strptime('20190111', '%Y%m%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline for a single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_logger()\n",
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-10'\n",
    "config['main']['end_date'] = '2019-01-10'\n",
    "\n",
    "retrieve_and_save_data_from_PACS(config)\n",
    "load_transform_and_save_data_from_files(config)\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_logger()\n",
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-09-05'\n",
    "config['main']['end_date'] = '2019-09-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrieve_and_save_single_day_data_from_PACS(config, datetime.datetime(2019, 9, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = extract_transform_and_save_data_from_files(config)\n",
    "#display(df)\n",
    "with pd.option_context(\"display.max_rows\", 1000): display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed_with_info = fetch_info_for_series(config, df_failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, df_failed_with_info], sort=True)\n",
    "df2.drop_duplicates('Series Instance UID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rescued_series = df_failed_with_info.copy()\n",
    "df_failed_series = df_rescued_series[\n",
    "            (df_rescued_series['Start Time'].isnull())\n",
    "            | (df_rescued_series['End Time'].isnull())\n",
    "            | (df_rescued_series['Machine'] == '')\n",
    "            | (df_rescued_series['Institution Name'] == '')]\n",
    "# exclude series where some information could still not be gathered (e.g. no end time or no machine)\n",
    "df_rescued_series = df_rescued_series.loc[~df_rescued_series.index.isin(df_failed_series.index), :]\n",
    "df2 = pd.concat([df, df_rescued_series], sort=True)\n",
    "df_failed_series\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-10'\n",
    "config['main']['end_date'] = '2019-01-10'\n",
    "df_studies = find_studies_for_day(config, '20190110', ['PT', 'NM'])\n",
    "df_studies = df_studies[df_studies['Patient ID'] == '2370187']\n",
    "df_all_series = find_series_for_studies(config, df_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_all_series[df_all_series['Number of Series Related Instances'].astype(int) > 1].Modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = df_all_series.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_series_subset = pd.concat([df_all_series[df_all_series.Modality == modality].head(n = 30) for modality in set(df_all_series.Modality)]).sort_values('Series Time').reset_index(drop=True)\n",
    "df_series = df_series_subset.copy()\n",
    "df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = fetch_info_for_series(config, df_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step before turing it to an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list of field names to extract for each modality\n",
    "to_fetch_fields_ctpt = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName',\n",
    "    'AcquisitionTime', 'Modality', 'ActualFrameDuration']\n",
    "to_fetch_fields_nm = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName',\n",
    "    'AcquisitionTime', 'Modality', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "# create modality specific masks of the DataFrame\n",
    "df_series_ctpt = df_series[df_series['Modality'].isin(['PT', 'CT'])]\n",
    "df_series_nm = df_series[df_series['Modality'] == 'NM']\n",
    "display(df_series_ctpt)\n",
    "display(df_series_nm)\n",
    "\n",
    "# prepare the CT/PT queries for the first instance (first image)\n",
    "query_dicts_ctpt = list(df_series_ctpt.apply(lambda row: {\n",
    "    'SeriesDate': row['Series Date'],\n",
    "    'PatientID': row['Patient ID'],\n",
    "    'SeriesInstanceUID': row['Series Instance UID'],\n",
    "    'InstanceNumber': '1'\n",
    "}, axis=1))\n",
    "# prepare the CT/PT queries for the last instance (last image)\n",
    "df_last_frames = df_series_ctpt[df_series_ctpt['Number of Series Related Instances'] != '1']\n",
    "if len(df_last_frames) > 0:\n",
    "    query_dicts_ctpt.extend(\n",
    "        df_last_frames.apply(lambda row: {\n",
    "            'SeriesDate': row['Series Date'],\n",
    "            'PatientID': row['Patient ID'],\n",
    "            'SeriesInstanceUID': row['Series Instance UID'],\n",
    "            'InstanceNumber': row['Number of Series Related Instances']\n",
    "        }, axis=1))\n",
    "# fetch the CT/PT data\n",
    "logging.info('Getting CT/PT data ({} queries)'.format(len(query_dicts_ctpt)))\n",
    "df_info_ctpt = get_data(config, query_dicts_ctpt, to_fetch_fields_ctpt)\n",
    "\n",
    "# prepare the NM queries for the first instance (first image)\n",
    "query_dicts_nm = list(df_series_nm.apply(lambda row: {\n",
    "    'SeriesDate': row['Series Date'],\n",
    "    'PatientID': row['Patient ID'],\n",
    "    'SeriesInstanceUID': row['Series Instance UID']\n",
    "}, axis=1))\n",
    "# fetch the NM data\n",
    "logging.info('Getting NM data ({} queries)'.format(len(query_dicts_nm)))\n",
    "df_info_nm = get_data(config, query_dicts_nm, to_fetch_fields_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 1000): display(df_info_ctpt.sort_values(['ManufacturerModelName', 'PatientID','SeriesInstanceUID', 'AcquisitionTime', 'InstanceNumber']))\n",
    "with pd.option_context(\"display.max_rows\", 1000): display(df_info_nm.sort_values(['ManufacturerModelName', 'PatientID', 'SeriesInstanceUID', 'Modality', 'AcquisitionTime', 'InstanceNumber']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the retrieved info DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_series_save = df_series.copy()\n",
    "df_info_ctpt_save = df_info_ctpt.copy()\n",
    "df_info_nm_save = df_info_nm.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually process the info and merge it back to the series DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series = df_series_save.copy()\n",
    "df_info_ctpt_save = df_info_ctpt_save.copy()\n",
    "df_info_nm_save = df_info_nm_save.copy()\n",
    "\n",
    "# Process PT/CT images\n",
    "if len(df_info_ctpt) > 0:\n",
    "\n",
    "    # get the images with a single instance\n",
    "    single_instances_UIDs = df_series.loc[\n",
    "        (df_series['Series Instance UID'].isin(df_info_ctpt['SeriesInstanceUID']))\\\n",
    "        & (df_series['Number of Series Related Instances'] == '1'), 'Series Instance UID']\n",
    "    logging.info('single_instances_UIDs')\n",
    "    display(single_instances_UIDs)\n",
    "    # duplicated them into the info DataFrame, so that they can also be merged together, as if there was two frames\n",
    "    df_info_ctpt_single_inst = df_info_ctpt[df_info_ctpt['SeriesInstanceUID'].isin(single_instances_UIDs)].copy()\n",
    "    df_info_ctpt_single_inst['InstanceNumber'] = 999999\n",
    "    df_info_ctpt_extended = pd.concat([df_info_ctpt, df_info_ctpt_single_inst], sort=True)\n",
    "    logging.info('df_info_ctpt_extended')\n",
    "    display(df_info_ctpt_extended)\n",
    "\n",
    "    # clean up the start times\n",
    "    df_info_ctpt_extended.loc[:, 'AcquisitionTime'] = df_info_ctpt_extended.loc[:, 'AcquisitionTime']\\\n",
    "        .apply(lambda t: str(t).split('.')[0])\n",
    "\n",
    "    # regroup the first and last instance rows on a single row\n",
    "    df_info_ctpt_merged = df_info_ctpt_extended[df_info_ctpt_extended['InstanceNumber'] == 1]\\\n",
    "        .merge(df_info_ctpt_extended[df_info_ctpt_extended['InstanceNumber'] > 1],\n",
    "               on=['SeriesInstanceUID', 'PatientID', 'ManufacturerModelName', 'Modality'],\n",
    "               suffixes=['_start', '_end'])\n",
    "    logging.info('df_info_ctpt_merged')\n",
    "    display(df_info_ctpt_merged)\n",
    "\n",
    "    # rename the columns and keep the appropriate ones\n",
    "    df_info_ctpt_clean = df_info_ctpt_merged.rename(columns={\n",
    "            'SeriesInstanceUID': 'Series Instance UID',\n",
    "            'PatientID': 'Patient ID',\n",
    "            'ManufacturerModelName': 'Machine',\n",
    "            'AcquisitionTime_start': 'Start Time',\n",
    "            'AcquisitionTime_end': 'End Time'})\\\n",
    "        .drop(columns=['InstanceNumber_start', 'InstanceNumber_end'])\n",
    "    logging.info('df_info_ctpt_clean')\n",
    "    display(df_info_ctpt_clean)\n",
    "    \n",
    "    s = pd.to_datetime(df_info_ctpt_clean['Start Time'], format='%H%M%S')\n",
    "    e = pd.to_datetime(df_info_ctpt_clean['End Time'], format='%H%M%S')\n",
    "    df_inv = df_info_ctpt_clean[s > e].copy()\n",
    "    df_inv[['Start Time','End Time']] = df_inv[['End Time','Start Time']]\n",
    "    df_info_ctpt_clean[s > e] = df_inv\n",
    "    logging.info('df_info_ctpt_clean 2')\n",
    "    display(df_info_ctpt_clean)\n",
    "    \n",
    "    # merge the info into the series DataFrame\n",
    "    df_series = df_series.merge(df_info_ctpt_clean, on=['Patient ID', 'Series Instance UID', 'Modality'], how='outer')\n",
    "    logging.info('df_series 1')\n",
    "    display(df_series)\n",
    "\n",
    "    # keep only the relevant columns\n",
    "    for f in ['Start Time', 'End Time', 'Machine']:\n",
    "        df_series[f] = df_series[f + '_y'].where(df_series[f + '_y'].notnull(), df_series[f + '_x'])\n",
    "        df_series.drop(columns=[f + '_y', f + '_x'], inplace=True)\n",
    "    logging.info('df_series post-CT')\n",
    "    display(df_series)\n",
    "\n",
    "# Process NM images\n",
    "if len(df_info_nm) > 0:\n",
    "    # clean up the start times\n",
    "    df_info_nm.loc[:, 'AcquisitionTime'] = df_info_nm.loc[:, 'AcquisitionTime']\\\n",
    "        .apply(lambda t: str(t).split('.')[0])\n",
    "    # use the AcquisitionTime as Start Time\n",
    "    df_info_nm['Start Time'] = df_info_nm['AcquisitionTime']\n",
    "    # call a function to calculate the End Times\n",
    "    df_info_nm['End Time'] = df_info_nm.apply(get_NM_series_end_time, axis=1)\n",
    "    # rename the columns and select the appropriate ones\n",
    "    df_info_nm_clean = df_info_nm.rename(columns={\n",
    "            'SeriesInstanceUID': 'Series Instance UID',\n",
    "            'PatientID': 'Patient ID',\n",
    "            'ManufacturerModelName': 'Machine'})\n",
    "    # merge the info into the series DataFrame\n",
    "    df_series = df_series.merge(df_info_nm_clean, on=['Patient ID', 'Series Instance UID', 'Modality'],\n",
    "        how='outer')\n",
    "    # keep only the relevant columns\n",
    "    for f in ['Start Time', 'End Time', 'Machine']:\n",
    "        df_series[f] = df_series[f + '_y'].where(df_series[f + '_y'].notnull(), df_series[f + '_x'])\n",
    "        df_series.drop(columns=[f + '_y', f + '_x'], inplace=True)\n",
    "    logging.info('df_series post-NM')\n",
    "    display(df_series)\n",
    "\n",
    "# remove duplicates\n",
    "df_series = df_series.drop_duplicates('Series Instance UID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series[['Patient ID', 'Modality', 'Start Time', 'End Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display(df_series)\n",
    "df_series.to_pickle('data/2019/2019-01/2019-01-10.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish the exploration after all this manual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "#df_series = load_data_from_files(config)\n",
    "# mark the rektakes and the machine group for each series\n",
    "df_series = mark_retakes(config, df_series)\n",
    "df_series = mark_machine_group(config, df_series)\n",
    "display(df_series[df_series['Patient ID'] == '2370187'])\n",
    "\n",
    "df_studies = df_series.replace(np.nan, '').groupby('SUID').agg({\n",
    "        'Series Date': lambda x: '/'.join(set(x)),\n",
    "        'Start Time': 'min',\n",
    "        'End Time': 'max',\n",
    "        'Study Description': lambda x: '/'.join(set(x)),\n",
    "        'Patient ID': lambda x: '/'.join(set(x)),\n",
    "        'Machine Group': lambda x: '/'.join(set(x)),\n",
    "        'Modality': lambda x: '/'.join(set(x)),\n",
    "        'Protocol Name': lambda x: '/'.join(set(x))\n",
    "    }).sort_values(['Series Date', 'Start Time', 'Machine Group', 'SUID'])\\\n",
    "    .rename(columns={'Series Date': 'Date'})\n",
    "\n",
    "df = df_studies[df_studies['Patient ID'] == '2370187'].copy()\n",
    "display(df)\n",
    "\n",
    "df['Machine'] = df['Machine Group'].str.replace('NoCT', '')\n",
    "df = df[df['Machine'] != 'mixed cases'].drop(columns='Machine Group')\n",
    "df.sort_values('Machine')[['Patient ID', 'Machine', 'Modality', 'Start Time', 'End Time', 'Study Description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test to query all images in one query data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all \"first\" CT/PT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_ctpt_first = []\n",
    "while len(df_info_ctpt_first) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_ctpt['Series Instance UID']))\n",
    "    ds.PatientID =  list(set(df_ctpt['Patient ID']))\n",
    "    ds.InstanceNumber = '1'\n",
    "    ds.Modality = ['CT','PT']\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_ctpt_first = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_info_ctpt_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all \"last\" CT/PT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_last = []\n",
    "while len(df_info_last) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_ctpt['Series Instance UID']))\n",
    "    ds.PatientID =  list(set(df_ctpt['Patient ID']))\n",
    "    ds.InstanceNumber = list(set(df_ctpt['Number of Series Related Instances']))\n",
    "    ds.Modality = ['CT','PT']\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_last = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_last.sort_values(by=\"InstanceNumber\").drop_duplicates(subset=[\"SeriesInstanceUID\"], keep=\"last\").reset_index()\n",
    "df_grouped.index += 1\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all NM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_nm = []\n",
    "while len(df_info_nm) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_nm['Series Instance UID'][0:5]))\n",
    "    ds.PatientID =  list(set(df_nm['Patient ID'][0:5]))\n",
    "    ds.Modality = 'NM'\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_nm = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Merge the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_series.merge(df_info.drop(columns='Modality').rename(columns={'SeriesInstanceUID': 'Series Instance UID'}), on='Series Instance UID', how='outer')\n",
    "#df_merged[['Series Date', 'Series Time', 'AcquisitionTime']]\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = []\n",
    "while len(df_info) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesInstanceUID = df_series['Series Instance UID']\n",
    "    ds.PatientID =  df_series['Patient ID']\n",
    "    ds.Modality = 'NM'\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info = get_data(config, [ds], to_fetch_fields)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
