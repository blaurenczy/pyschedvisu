{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for retrieving data from PACS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import logging\n",
    "import configparser\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from IPython.core import display as ICD\n",
    "\n",
    "from pydicom.dataset import Dataset\n",
    "\n",
    "from scripts.run_all import run_all\n",
    "from scripts.retrieve_data_from_PACS import *\n",
    "\n",
    "# set the width of display to infinite for all pandas DataFrame\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "# set the level of pynetdicom module's logger to ERROR, to avoid any logs\n",
    "logging.getLogger('pynetdicom').setLevel(logging.ERROR)\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the \"config\" object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all 'PT' and 'NM' studies for a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_studies = find_studies_for_day(config, config['main']['start_date'], ['PT', 'NM'])\n",
    "\n",
    "# filter out irrelevant studies\n",
    "df_studies = df_studies[df_studies['Patient ID'].str.match('^\\d+$')]\n",
    "df_studies = df_studies[~df_studies['Study Description'].isin(['EXTRINSEQUE'])]\n",
    "df_studies.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all series for the found studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this DataFrame stores the list of all the series found for all studies\n",
    "df_all_series = pd.DataFrame()\n",
    "\n",
    "# go through each study\n",
    "logging.info('Going through {} studie(s)'.format(len(df_studies)))\n",
    "for i_study in range(len(df_studies)):\n",
    "    \n",
    "    # find all series of the current study\n",
    "    df_series = find_series_for_study(config, df_studies.iloc[i_study])\n",
    "    if df_series is None:\n",
    "        logging.warning('Skipping study because there are no usable Series inside')\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # filter for the institution name\n",
    "    accepted_inst_names = config['extract']['accepted_institution_names'].split('\\n')\n",
    "    inst_names = list(set([inst_name.replace('  ', ' ') for inst_name in df_series.loc[:, 'Institution Name']]))\n",
    "    if len(inst_names) > 1:\n",
    "        logging.warning('Multiple institution names for study: \"{}\"'.format(' / '.join(inst_names)))\n",
    "        inst_name = 'mixed'\n",
    "    else:\n",
    "        inst_name = inst_names[0]\n",
    "    df_studies.loc[i_study, 'Institution Name'] = inst_name\n",
    "    if inst_name.lower().replace(' ', '') not in accepted_inst_names:\n",
    "        logging.warning('Skipping study because it is not from CHUV (but from \"{}\")'.format(inst_name))\n",
    "        continue\n",
    "    \n",
    "    # append the new series to the main series DataFrame\n",
    "    df_all_series = df_all_series.append(df_series, sort=False, ignore_index=True)\n",
    "    if inst_name == 'mixed':\n",
    "        ICD.display(df_series)\n",
    "    \n",
    "df_all_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check whether there are any studies/series with mixed \"Institution Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_series[df_all_series['Study Instance UID'].isin(df_studies[df_studies['Institution Name'] == 'mixed']['Study Instance UID'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through each series and find information about them, allowing re-tries if a Series cannot be immediately fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_series_save = df_all_series.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each study\n",
    "logging.info('Going through {} series'.format(len(df_all_series)))\n",
    "df_all_series['i_try'] = None\n",
    "n_max_try = 10\n",
    "for i_series in df_all_series.index:\n",
    "    \n",
    "    row_info = None\n",
    "    i_try = 0\n",
    "    while row_info is None:\n",
    "        i_try += 1\n",
    "        df_all_series.loc[i_series, 'i_try'] = i_try\n",
    "        # find information about this series by fetching some images\n",
    "        row_info = fetch_info_for_series(config, df_all_series.loc[i_series])\n",
    "        # if there is no data and we reached our maximum number of tries\n",
    "        if row_info is None and i_try >= n_max_try:\n",
    "            # mark row as a failed trial and abort\n",
    "            df_all_series.loc[i_series, 'i_try'] = -1\n",
    "            break\n",
    "        # if there is no data but we did not reach (yet) our maximum number of tries\n",
    "        elif row_info is None:\n",
    "            # delay the next retry\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "    # abort processing for this series no data\n",
    "    if row_info is None:\n",
    "        logging.error('ERROR with series {}: no data found'.format(df_all_series.loc[i_series, 'Series Instance UID']))\n",
    "        continue\n",
    "\n",
    "    # copy the relevant parameters into the main DataFrame\n",
    "    df_all_series.loc[i_series, 'start_time'] = row_info['start_time']\n",
    "    df_all_series.loc[i_series, 'end_time'] = row_info['end_time']\n",
    "    df_all_series.loc[i_series, 'machine'] = row_info['machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some statistics on the success / failure rates of fetching info for SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_try = df_all_series['i_try']\n",
    "n = len(df_all_series)\n",
    "failures = i_try[i_try == -1]\n",
    "successes = i_try[i_try > 0]\n",
    "first_tries = successes[successes == 1]\n",
    "multi_tries = successes[successes > 1]\n",
    "logging.info('Success:     {:03d} / {:03d} ({:.1f}%)'.format(len(successes), n, 100 * len(successes) / n))\n",
    "logging.info('Failures:    {:03d} / {:03d} ({:.1f}%)'.format(len(failures), n, 100 * len(failures) / n))\n",
    "logging.info('First tries: {:03d} / {:03d} ({:.1f}%)'.format(len(first_tries), len(successes), 100 * len(first_tries) / len(successes)))\n",
    "logging.info('Multi-tries: {:03d} / {:03d} ({:.1f}%)'.format(len(multi_tries), len(successes), 100 * len(multi_tries) / len(successes)))\n",
    "logging.info('Mean ± SD multi-tries: {:.2f} ± {:.2f}'.format(multi_tries.mean(), multi_tries.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through the failed series again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_series_save2 = df_all_series.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# go through each study\n",
    "logging.info('Going through {} series'.format(len(df_all_series)))\n",
    "df_all_series['i_try2'] = None\n",
    "n_max_try = 10\n",
    "failed_indices = df_all_series[df_all_series['i_try'] == -1].index\n",
    "for i_series in failed_indices:\n",
    "    \n",
    "    row_info = None\n",
    "    i_try = 0\n",
    "    while row_info is None:\n",
    "        i_try += 1\n",
    "        df_all_series.loc[i_series, 'i_try2'] = i_try\n",
    "        # find information about this series by fetching some images\n",
    "        row_info = fetch_info_for_series(config, df_all_series.loc[i_series])\n",
    "        if row_info is None and i_try >= n_max_try:\n",
    "            df_all_series.loc[i_series, 'i_try2'] = -1\n",
    "            break\n",
    "            \n",
    "    # abort processing for this series no data\n",
    "    if row_info is None:\n",
    "        logging.error('ERROR with series {}: no data found'.format(df_all_series.loc[i_series, 'Series Instance UID']))\n",
    "        continue\n",
    "\n",
    "    # copy the relevant parameters into the main DataFrame\n",
    "    df_all_series.loc[i_series, 'start_time'] = row_info['start_time']\n",
    "    df_all_series.loc[i_series, 'end_time'] = row_info['end_time']\n",
    "    df_all_series.loc[i_series, 'machine'] = row_info['machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some statistics on the success / failure rates of fetching info for SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_try = df_all_series['i_try']\n",
    "i_try2 = df_all_series['i_try2']\n",
    "n = len(df_all_series)\n",
    "failures = i_try[i_try == -1]\n",
    "successes = i_try[i_try != -1]\n",
    "first_tries = successes[successes == 1]\n",
    "multi_tries = successes[successes > 1]\n",
    "recoveries = i_try2[(i_try == -1) & (i_try2 != -1)]\n",
    "total_failures = i_try2[(i_try == -1) & (i_try2 == -1)]\n",
    "recov_first_tries = recoveries[recoveries == 1]\n",
    "recov_multi_tries = recoveries[recoveries > 1]\n",
    "logging.info('Success    (1):     {:03d} / {:03d} ({:.1f}%)'.format(len(successes), n, 100 * len(successes) / n))\n",
    "logging.info('Failures    (1):    {:03d} / {:03d} ({:.1f}%)'.format(len(failures), n, 100 * len(failures) / n))\n",
    "logging.info('First tries (1): {:03d} / {:03d} ({:.1f}%)'.format(len(first_tries), len(successes), 100 * len(first_tries) / len(successes)))\n",
    "logging.info('Multi-tries (1): {:03d} / {:03d} ({:.1f}%)'.format(len(multi_tries), len(successes), 100 * len(multi_tries) / len(successes)))\n",
    "logging.info('Mean ± SD multi-tries (1): {:.2f} ± {:.2f}'.format(multi_tries.mean(), multi_tries.std()))\n",
    "logging.info('Recoveries  (2):  {:03d} / {:03d} ({:.1f}%)'.format(len(recoveries), len(failures), 100 * len(recoveries) / len(failures)))\n",
    "logging.info('Total fails (2):  {:03d} / {:03d} ({:.1f}%)'.format(len(total_failures), len(failures), 100 * len(total_failures) / len(failures)))\n",
    "logging.info('First tries (2): {:03d} / {:03d} ({:.1f}%)'.format(len(recov_first_tries), len(recoveries), 100 * len(recov_first_tries) / len(recoveries)))\n",
    "logging.info('Multi-tries (2): {:03d} / {:03d} ({:.1f}%)'.format(len(recov_multi_tries), len(recoveries), 100 * len(recov_multi_tries) / len(recoveries)))\n",
    "logging.info('Mean ± SD multi-tries (2): {:.2f} ± {:.2f}'.format(recov_multi_tries.mean(), recov_multi_tries.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many failed series we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_start_time = df_all_series[df_all_series['start_time'].isnull()]\n",
    "df_with_start_time = df_all_series[~df_all_series['start_time'].isnull()]\n",
    "logging.info('{}/{} rows with start time ({:.1f} %), {}/{} rows without start time ({:.1f} %)'.format(\n",
    "    len(df_with_start_time), len(df_all_series), 100 * len(df_with_start_time) / len(df_all_series),\n",
    "    len(df_no_start_time), len(df_all_series), 100 * len(df_no_start_time) / len(df_all_series)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the 2 latest series for each study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_series = df_all_series_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study_UIDs = list(set(df_all_series['Study Instance UID']))\n",
    "logging.info('Found {} unique study UIDs'.format(len(study_UIDs)))\n",
    "\n",
    "FMT = '%H%M%S'\n",
    "# put a threshold at 59 minutes\n",
    "study_split_thresh = 59 * 60\n",
    "\n",
    "# create a column to mark the second-take series\n",
    "df_all_series['second_take'] = False\n",
    "\n",
    "# build the list of rows to keep\n",
    "indices_to_keep = []\n",
    "for i in range(len(study_UIDs)):\n",
    "    \n",
    "    # get the series related to the current Study Instance UID\n",
    "    sUID = study_UIDs[i]\n",
    "    series_str = '[{:2d}/{:2d}] {}...{}'.format(i, len(study_UIDs) - 1, sUID[:8], sUID[-5:])\n",
    "    df_series_for_study = df_all_series[df_all_series['Study Instance UID'] == sUID]\n",
    "    logging.debug('Processing {}: found {:2d} series'.format(series_str, len(df_series_for_study)))\n",
    "    \n",
    "    # extract the list of indices for the current series\n",
    "    indices_for_study = list(df_series_for_study.index.values)\n",
    "    \n",
    "    # sort according to time and keep the last 2 indices (last 2 series)\n",
    "    df_series_for_study = df_series_for_study.sort_values('Series Time')\n",
    "    indices_to_add = df_series_for_study.iloc[-2:,:].index.values\n",
    "    logging.debug(indices_to_add)\n",
    "    indices_to_keep.extend(list(indices_to_add))\n",
    "    \n",
    "    # there must be at least 2 series for any splitting\n",
    "    if len(df_series_for_study) < 2:\n",
    "        continue\n",
    "    \n",
    "    # check if there is any big difference in the successive series time\n",
    "    time_diff = df_series_for_study['Series Time'].apply(lambda t: dt.strptime(t, FMT)).diff()\n",
    "    time_diff_sec = time_diff[1:].apply(lambda td: td.seconds)\n",
    "    # get the DataFrame indices where the time difference is bigger than the threshold\n",
    "    split_indices = list(time_diff_sec[time_diff_sec > study_split_thresh].index)\n",
    "    \n",
    "    # if there is no splitting indices, keep the last 2 series\n",
    "    if len(split_indices) == 0:\n",
    "        logging.debug('No splitting required for study {}'.format(series_str))\n",
    "        continue\n",
    "    \n",
    "    # if there is more than one split point, throw an error and do not do any splitting\n",
    "    elif len(split_indices) > 1:\n",
    "        logging.error('Error: too many splitting points found for study {}: [{}]'\n",
    "                      .format(series_str, ', '.join(str(split_indices))))\n",
    "        continue\n",
    "    \n",
    "    # if there is only a single splitting time\n",
    "    else:\n",
    "        \n",
    "        indices_to_add = [\n",
    "            max(split_indices[0] - 2, min(indices_for_study)),\n",
    "            max(split_indices[0] - 1, min(indices_for_study))\n",
    "        ]\n",
    "        indices_to_add = list(set(indices_to_add))\n",
    "        logging.info(indices_to_add)\n",
    "        indices_to_keep.extend(indices_to_add)\n",
    "        logging.info('Splitting {} at {}'.format(series_str, split_indices[0]))\n",
    "        df_all_series.loc[[i for i in indices_for_study if i >= split_indices[0]], 'second_take'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the relevant rows\n",
    "df_series_pruned = df_all_series.loc[indices_to_keep].sort_values(['Patient ID', 'Series Time'])\n",
    "\n",
    "# create a column of unique ID (including the information about second takes)\n",
    "df_series_pruned['UID'] = ''\n",
    "i_UID = 0\n",
    "\n",
    "# create a unique ID for the relevant series\n",
    "for ind in df_series_pruned.index:\n",
    "    UID = '{}-{}'.format(*df_series_pruned.loc[ind, ['Series Date', 'Patient ID']])\n",
    "    UID += '-{:04d}-A'.format(i_UID)\n",
    "    i_UID += 1\n",
    "    df_series_pruned.loc[ind, 'UID'] = UID\n",
    "\n",
    "df_series_pruned.iloc[:,[0,1,2,3,6,12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a summary of what machines are used in which institution names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for series where information could be gathered\n",
    "df_series_filtered = df_all_series[~df_all_series.start_time.isnull()]\n",
    "# get a list of machines\n",
    "machines = set(df_series_filtered.machine[~df_series_filtered.machine.isnull()])\n",
    "logging.info('Machines:\\n  - {}'.format('\\n  - '.join(machines)))\n",
    "# create groups of machines\n",
    "groups = []\n",
    "# create a DataFrame that regroups all the sub-DataFrames\n",
    "all_grouped_df = None\n",
    "# go through the list of machines\n",
    "for machine in machines:\n",
    "    # get all the study IDs for this machine\n",
    "    study_UIDs = set(df_series_filtered[df_series_filtered.machine == machine]['Study Instance UID'])\n",
    "    rows_with_same_study_UIDs = df_series_filtered['Study Instance UID'].isin(study_UIDs)\n",
    "    sub_df = df_series_filtered[rows_with_same_study_UIDs].loc[:, ['machine', 'Institution Name', 'Study Instance UID']] \\\n",
    "        .rename(columns = {'machine': 'Machine Name', 'Study Instance UID': 'Number of Series'})\n",
    "    sub_df_grouped = sub_df.groupby(['Machine Name', 'Institution Name']).count()\n",
    "    logging.debug(sub_df)\n",
    "    group_name = ', '.join(sorted(list(set(sub_df['Machine Name']))))\n",
    "    if group_name in groups: continue\n",
    "    for conf_machine in config['machines']:\n",
    "        logging.info('conf_machine: {}'.format(conf_machine))\n",
    "        for conf_machine_pattern in config['machines'][conf_machine].split(','):\n",
    "            if re.match(conf_machine_pattern, machine, re.IGNORECASE):\n",
    "                logging.info('match')\n",
    "    sub_df_grouped['Machine Group'] = i_group\n",
    "    i_group += 1\n",
    "    groups.append(group_name)\n",
    "    if all_grouped_df is None:\n",
    "        all_grouped_df = sub_df_grouped\n",
    "    else:\n",
    "        all_grouped_df = pd.concat([all_grouped_df, sub_df_grouped])\n",
    "    ICD.display(sub_df_grouped)\n",
    "logging.debug(set(groups))\n",
    "all_grouped_df.groupby(['Machine Group', 'Machine Name', 'Institution Name']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a summary of what machines are used together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter for series where information could be gathered\n",
    "df_series_filtered = df_all_series[~df_all_series.start_time.isnull()]\n",
    "# get a list of machines\n",
    "machines = set(df_series_filtered.machine[~df_series_filtered.machine.isnull()])\n",
    "logging.info('Machines:\\n  - {}'.format('\\n  - '.join(machines)))\n",
    "# create groups of machines\n",
    "groups = []\n",
    "# create a DataFrame that regroups all the sub-DataFrames\n",
    "all_grouped_df = None\n",
    "# go through the list of machines\n",
    "for machine in machines:\n",
    "    # get all the study IDs for this machine\n",
    "    study_UIDs = set(df_series_filtered[df_series_filtered.machine == machine]['Study Instance UID'])\n",
    "    rows_with_same_study_UIDs = df_series_filtered['Study Instance UID'].isin(study_UIDs)\n",
    "    sub_df = df_series_filtered[rows_with_same_study_UIDs].loc[:, ['machine', 'Modality', 'Study Instance UID']] \\\n",
    "        .rename(columns = {'machine': 'Machine Name', 'Study Instance UID': 'Number of Series'})\n",
    "    sub_df_grouped = sub_df.groupby(['Machine Name', 'Modality']).count()\n",
    "    logging.debug(sub_df_grouped.index)\n",
    "    group_name = ', '.join(sorted(list(set(sub_df['Machine Name']))))\n",
    "    if group_name in groups: continue\n",
    "    for conf_machine in config['machines']:\n",
    "        logging.info('conf_machine: {}'.format(conf_machine))\n",
    "        for conf_machine_pattern in config['machines'][conf_machine].split(','):\n",
    "            if re.match(conf_machine_pattern, machine, re.IGNORECASE):\n",
    "                logging.info('match')\n",
    "    sub_df_grouped['Machine Group'] = i_group\n",
    "    i_group += 1\n",
    "    groups.append(group_name)\n",
    "    if all_grouped_df is None:\n",
    "        all_grouped_df = sub_df_grouped\n",
    "    else:\n",
    "        all_grouped_df = pd.concat([all_grouped_df, sub_df_grouped])\n",
    "    #ICD.display(sub_df_grouped)\n",
    "logging.debug(set(groups))\n",
    "all_grouped_df.groupby(['Machine Group', 'Machine Name', 'Modality']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through failed series again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# go through each study\n",
    "logging.info('Going through {} series'.format(len(df_series_pruned[df_series_pruned['start_time'].isnull()])))\n",
    "for i_series in df_series_pruned[df_series_pruned['start_time'].isnull()].index:\n",
    "    \n",
    "    # find information about this series by fetching some images\n",
    "    row_info = fetch_info_for_series(config, df_series_pruned.loc[i_series])\n",
    "\n",
    "    # abort processing for this series no data\n",
    "    if row_info is None:\n",
    "        logging.error('ERROR with series {}: no data found'.format(df_series_pruned.loc[i_series, 'Series Instance UID']))\n",
    "        continue\n",
    "\n",
    "    # copy the relevant parameters into the main DataFrame\n",
    "    df_series_pruned.loc[i_series, 'start_time'] = row_info['start_time']\n",
    "    df_series_pruned.loc[i_series, 'end_time'] = row_info['end_time']\n",
    "    df_series_pruned.loc[i_series, 'machine'] = row_info['machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Remove failed series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_pruned[df_series_pruned['start_time'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune series by time overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_UIDs = list(set(df_all_series['Study Instance UID']))\n",
    "logging.info('Found {} unique study UIDs'.format(len(study_UIDs)))\n",
    "\n",
    "# build the list of rows to exclude\n",
    "indices_to_exclude = []\n",
    "for i in range(len(study_UIDs)):\n",
    "    \n",
    "    # get the series related to the current Study Instance UID\n",
    "    sUID = study_UIDs[i]\n",
    "    df_series_for_study = df_all_series[df_all_series['Study Instance UID'] == sUID]\n",
    "    logging.info('Processing [{:2d}/{:2d}] {}...{}: found {:2d} series'.format(i, len(study_UIDs) - 1,\n",
    "        sUID[:8], sUID[-5:], len(df_series_for_study)))\n",
    "    \n",
    "    # prune the series by time overlap\n",
    "    df_series_for_study_pruned = prune_by_time_overlap(df_series_for_study)\n",
    "    logging.info(len(df_series_for_study_pruned))\n",
    "    \n",
    "    \"\"\"\n",
    "    to_exclude_rows = df_series['Series Description'].str.match(descr_pattern, case=False)\n",
    "    # gather all the indices\n",
    "    indices_to_exclude.append(to_exclude_rows[to_exclude_rows == True].index)\n",
    "# flatten the list\n",
    "indices_to_exclude = [index for indices in indices_to_exclude for index in indices.values]\n",
    "# if there is something to exclude, show a message and drop the rows\n",
    "if len(indices_to_exclude) > 0:\n",
    "    logging.debug('Found {} series to exclude based on their description: \"{}\"'.format(len(indices_to_exclude),\n",
    "        '\", \"'.join(df_series.loc[indices_to_exclude]['Series Description'])))\n",
    "    df_series.drop(indices_to_exclude, inplace=True)\n",
    "logging.debug('Found {} series after filtering description'.format(len(df_series)))\n",
    "df_series = prune_by_time_overlap(df_series)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all series for the found studies and get their time ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the list of accepted (valid) institution names from the config\n",
    "accepted_inst_names = config['extract']['accepted_institution_names'].split('\\n')\n",
    "    \n",
    "# this DataFrame stores the list of all the series found for all studies\n",
    "df_all_series = pd.DataFrame()\n",
    "\n",
    "# go through each study\n",
    "logging.info('Going through {} studie(s)'.format(len(df_studies)))\n",
    "#for i_study in df_studies.index:\n",
    "for i_study in df_studies.index[0:1]:\n",
    "    \n",
    "    # find all series of the current study\n",
    "    df_series = find_series_for_study(config, df_studies.loc[i_study, :])\n",
    "    \n",
    "    # filter for the institution name\n",
    "    inst_name = df_series.loc[0, 'Institution Name'].replace('  ', ' ')\n",
    "    df_studies.loc[i_study, 'Institution Name'] = inst_name\n",
    "    if inst_name.lower().replace(' ', '') not in accepted_inst_names:\n",
    "        logging.warning('  Skipping study because it is not from CHUV (institution name: \"{}\")'.format(inst_name))\n",
    "        df_studies.drop(i_study, inplace=True)\n",
    "        continue\n",
    "    \n",
    "    # go through each series\n",
    "    logging.info('Going through {} series'.format(len(df_series)))\n",
    "    #for i_series in df_series.index:\n",
    "    for i_series in df_series.index[2:3]:\n",
    "        \n",
    "        # find information about this series by fetching some images\n",
    "        row_info = fetch_info_for_series(config, df_series.loc[i_series, :])\n",
    "        \n",
    "        # abort processing for this series no data\n",
    "        if row_info is None:\n",
    "            logging.error('  Skipping series {}: no data found.'.format(df_series.loc[i_series, 'Series Instance UID']))\n",
    "            continue\n",
    "\n",
    "        raise(TypeException, 'stop')\n",
    "            \n",
    "        # copy the relevant parameters into the main DataFrame\n",
    "        df_series.loc[i_series, 'start_time'] = row_info['start_time']\n",
    "        df_series.loc[i_series, 'end_time'] = row_info['end_time']\n",
    "        df_series.loc[i_series, 'machine'] = row_info['machine']\n",
    "    \n",
    "    if 'start_time' not in df_series.columns:\n",
    "        logging.error('  Skipping study {} because no valid series was found.'\n",
    "                      .format(df_series.loc[i_series, 'Study Instance UID']))\n",
    "        continue\n",
    "        \n",
    "    # remove redundant series\n",
    "    df_series = prune_by_time_overlap(df_series)\n",
    "    \n",
    "    # append the new series to the main series DataFrame\n",
    "    df_all_series = df_all_series.append(df_series, sort=False, ignore_index=True)\n",
    "    \n",
    "logging.debug(\"\"\"\n",
    "    # create time ranges from the start/end times\n",
    "    time_ranges = []\n",
    "    for i_serie in range(len(df_series)):\n",
    "        time_ranges.append('{}-{}'.format(df_series.loc[i_serie, 'start_time'],\n",
    "                                          df_series.loc[i_serie, 'end_time']))\n",
    "    \n",
    "    # propagate back the time range and machine name information\n",
    "    df_studies.loc[i_study, 'machines'] = ','.join(list(set(df_series['machine'])))\n",
    "    df_studies.loc[i_study, 'time_ranges'] = ','.join(time_ranges)\n",
    "    df_studies.loc[i_study, 'overall_time_range'] = '{}-{}'.format(\n",
    "        df_series.iloc[0]['start_time'], df_series.iloc[-1]['end_time'])\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_studies\n",
    "df_series.iloc[:, [0,3,4,5,6,7,13,14,15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for a study with a \"reprise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.info(df_studies.iloc[2, :])\n",
    "#df_series = find_series_for_study(config, df_studies.iloc[2, :])\n",
    "ICD.display(df_series.iloc[:, [1,3,4,5,6,7,11,12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_studies.to_pickle(config['main']['start_date'] + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further process the studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_studies.loc[:, ['Study Date', 'Study Description', 'Patient ID', 'machines']]\n",
    "df.columns = ['date', 'descr', 'pid', 'machine']\n",
    "df['start_time'] = df_studies.loc[:, 'Study Time']\n",
    "df['end_time'] = df_studies.loc[:, 'overall_time_range'].apply(lambda s: s.split('-')[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the machines to have some consensus\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_names = ['Vision 600', 'Discovery 690', 'Millennium MPR', 'Intevo 16', 'Discovery 670']\n",
    "\n",
    "\"\"\"\n",
    "Biograph64/vision PT\n",
    "discovery 690 PT\n",
    "*discovery 670 SPECT\n",
    "Millennium SPECT sans CT\n",
    "Encore2/Intevo SPECT\n",
    "\"\"\"\n",
    "\n",
    "for machine_name in machine_names:\n",
    "    matching_rows = df['machine'].str.match('.*' + machine_name + '.*', case=False)\n",
    "    if matching_rows.sum() > 0:\n",
    "        logging.info('Found {} rows matching the name \"{}\":'.format(matching_rows.sum(), machine_name))\n",
    "    df.loc[matching_rows, 'machine'] = machine_name\n",
    "\n",
    "# replace the \"Encore2\" machine name to \"Intevo\", since it is the same machine\n",
    "#df.loc[df['machine'] == 'Encore2', 'machine'] = 'Intevo 16'\n",
    "#machine_names.remove('Encore2')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the descriptions to have some consensus\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_patterns = {'FDG Corps Entier': 'fdgcorpsentier', 'FDG Tronc': 'fdgtronc', 'Rb82 Coeur': 'rb82coeur',\n",
    "                       'FDG Abdomen TAP Veineux Corps Entier': 'abdomen1fdgtapveineuxpetcorpsentierflowadult',\n",
    "                       'Scintigraphie OctreoScan': 'scintioctreoscan', 'FDG WB Child': 'pet1petfdgwbflowchild'}\n",
    "for descr in description_patterns.keys():\n",
    "    matching_rows = df['descr'].str.lower().str.replace('[-_^ ()]', '').str\\\n",
    "        .match('.*' + description_patterns[descr] + '.*', case=False)\n",
    "    if matching_rows.sum() > 0:\n",
    "        logging.info('Found {} rows matching the name \"{}\":'.format(matching_rows.sum(), descr))\n",
    "    df.loc[matching_rows, 'descr'] = descr\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for overlap for each machine\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for machine in machine_names:\n",
    "    logging.info('Checking overlap for machine \"{}\"'.format(machine))\n",
    "    df_machine = df[df['machine'] == machine]\n",
    "    df[df['machine'] == machine] = prune_by_time_overlap(df_machine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(set(df['machine']))\n",
    "df.sort_values('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save processed data\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle(config['main']['start_date'] + '_processed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle('20190806.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_times = df2['time_ranges'].str.split(',').apply(lambda tr1: [tr2.split('-')[1] for tr2 in tr1[-2:]])\n",
    "end_times = end_times[end_times.apply(len) == 2]\n",
    "end_times.apply(lambda l: [i[0] for i in sorted(enumerate(l), key=lambda x:x[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_times = df2['time_ranges'].str.split(',').apply(lambda tr1: [tr2.split('-')[1] for tr2 in tr1[-2:]])\n",
    "end_times = end_times[end_times.apply(len) == 2]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
