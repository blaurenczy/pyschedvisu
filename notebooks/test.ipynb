{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import os\n",
    "os.chdir('C:/TEMP/pySchedVisu/code')\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from main import run, load_config\n",
    "from utils import get_day_range\n",
    "from retrieve_data import *\n",
    "from extract_data import load_transform_and_save_data_from_files, add_preparation_times\n",
    "from create_report import create_report, get_report_type\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set the width of the notebook\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displays for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20160101'\n",
    "config['main']['end_date'] = '20191227'\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies[['Date', 'Patient ID', 'Machine', 'Machine Group', 'Modality', 'Description', 'Study Description', 'Start Time', 'End Time', 'Start Time Prep', 'End Time Prep']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('max_colwidth', 29): display(df_series[['Date', 'Patient ID', 'Machine', 'Machine Group List', 'Machine Group', 'Modality', 'Institution Name', 'Series Time', 'Start Time',\n",
    "       'End Time', 'Protocol Name', 'Study Description', 'i_take', 'SUID', 'Study Instance UID', 'Series Instance UID']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Exclude bad series descriptions #66\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_list = \"\"\"<ALPHA Range[1]>\"\n",
    "\"<ALPHA Range[2]>\"\n",
    "\"<ALPHA Range>\"\n",
    "\"<MIP Range>\"\n",
    "\"1.25 BONE\"\n",
    "\"177-Lu CONTROLE POST-TRAITEMENT [R\"\n",
    "\"177-Lu CONTROLE POST-TRAITEMENT [Recon Flash3D - AC - AC ]\"\n",
    "\"177-Lu CONTROLE POST-TRAITEMENT [Recon Flash3D - AC ]\"\n",
    "\"177-Lu CONTROLE POST-TRAITEMENT [Recon Flash3D - NoAC - NoAC ]\"\n",
    "\"177-Lu CONTROLE POST-TRAITEMENT [Recon Flash3D - NoAC ]\"\n",
    "\"177-Lu CONTROLE POST-TRAITEMENT [Recon xSPECT - AC ]\"\n",
    "\"1MIP\"\n",
    "\"1Processed Images LUNG\"\n",
    "\"1Processed Images\"\n",
    "\"2 CT XBONE  1.5  B31s STD\"\n",
    "\"2 CT XBONE  1.5  B70s OS\"\n",
    "\"2 CT XBONE  1.5  eFoV OS\"\n",
    "\"2Processed Images LUNG\"\n",
    "\"3D Saved State - AutoSave\"\n",
    "\"Abdomen LD  2.0  B31s STD\"\n",
    "\"AC  2 CT XBONE  2.0  B31s\"\n",
    "\"AC  3 CT XQUANT  2.0  B31s\"\n",
    "\"AC  Abdomen LD  2.0  B31s\"\n",
    "\"AC  Abdomen LD  3.0  I31s\"\n",
    "\"AC  Bassin xB 2.0  B31s\"\n",
    "\"AC  CT Cerveau  2.0  HD_FoV\"\n",
    "\"AC  CT Foie  2.0  HD_FoV\"\n",
    "\"AC  CT LU177  3.0  I31s\"\n",
    "\"AC  CT Thorax  2.0  HD_FoV\"\n",
    "\"AC  CT Thorax  2.0 BH HD_FoV\"\n",
    "\"AC  CT Tronc  2.0  HD_FoV\"\n",
    "\"AC  CT TRONC  2.0  HD_FoV\"\n",
    "\"AC  CT Tronc 2.0  HD_FoV\"\n",
    "\"AC  CT Tronc low  2.0  HD_FoV\"\n",
    "\"AC  CT Tronc Low  2.0  HD_FoV\"\n",
    "\"AC  CT WB  3.0  HD_FoV\"\n",
    "\"AC  CT WB low  2.0  HD_FoV\"\n",
    "\"AC  HCHprot xbone  2.0  B31s\"\n",
    "\"AC  HCHprot xbone  2.0  I31s\"\n",
    "\"AC  TetecouF50xB  2.0  B31s\"\n",
    "\"AC  Thorax LD  3.0  I31s\"\n",
    "\"AC CT  2.0  HD_FoV\"\n",
    "\"AC CT 2.0  HD_FoV\"\n",
    "\"AC CT Cardiac  3.0   HD_FoV\"\n",
    "\"AC CT Tronc  2.0   HD_FoV\"\n",
    "\"AC CT Tronc  2.0  HD_FoV\"\n",
    "\"AC CT WB  2.0  HD_FoV\"\n",
    "\"AC CT WB low  2.0   HD_FoV\"\n",
    "\"AW electronic pages\"\n",
    "\"AX\"\n",
    "\"AXIAL FD MAA\"\n",
    "\"AXIAL FUSE\"\n",
    "\"Axial Fused\"\n",
    "\"AXIAL FUSED\"\n",
    "\"Axial Fusion-\"\n",
    "\"AXIAL MAA\"\n",
    "\"AXIAL\"\n",
    "\"axial2\"\n",
    "\"AXIALE\"\n",
    "\"AXIALES\"\n",
    "\"AxialShuntIntestinale\"\n",
    "\"Bassin xB 1.0  B31s STD\"\n",
    "\"Bassin xB 1.0  B70s BONE\"\n",
    "\"Body  3.0  I31s AC\"\n",
    "\"COR MN\"\n",
    "\"COR\"\n",
    "\"coro\"\n",
    "\"CORONAL FD MAA\"\n",
    "\"coronal\"\n",
    "\"CORONAL\"\n",
    "\"CORONALE\"\n",
    "\"Coronales FUSION 2\"\n",
    "\"CORONALS maa\"\n",
    "\"COU STD ceCT\"\n",
    "\"CT  RECON  OS 1.25\"\n",
    "\"CT 0.625 Pied Drt\"\n",
    "\"CT 1.25 OS\"\n",
    "\"CT 1.25 RECON BONE\"\n",
    "\"CT 2.0  I30f  3\"\n",
    "\"CT ABDO-PELV 2.5 STD\"\n",
    "\"CT ABDOPELV 70S 2.5\"\n",
    "\"CT Bassin Bone 1.25\"\n",
    "\"CT BASSIN REC1.25 STD ASIR40\"\n",
    "\"CT Bone 1.25 ASIR40\"\n",
    "\"CT BONE RECON 1.25\"\n",
    "\"CT Bone+\"\n",
    "\"CT CENTRAGE 3.75 STD ASIR40\"\n",
    "\"CT CENTRAGE L4-L5\"\n",
    "\"CT CENTRAGE ORL 1.25 mm\"\n",
    "\"CT CENTRAGE ORL\"\n",
    "\"CT Cerveau 1.25 STD ASIR40\"\n",
    "\"CT Cerveau 1.25mm DETAIL\"\n",
    "\"CT Cerveau 1.2mm BONE CORR\"\n",
    "\"CT Cerveau 3.75 STD ASIR40\"\n",
    "\"CT Cerveau 5mm IV ASIR40 CORR\"\n",
    "\"CT Chev Drt Cor\"\n",
    "\"CT Chev Drt Sag\"\n",
    "\"Ct Chev Drt Trans\"\n",
    "\"CT COU RECON\"\n",
    "\"CT COU-ORL 1.25 STD\"\n",
    "\"CT FCH 10MIN LUNG 1.25\"\n",
    "\"CT Foie  2.0  I30f\"\n",
    "\"CT GA68 TRONC 60MIN recon os\"\n",
    "\"CT Lowdose 3.75 STD ASIR40\"\n",
    "\"CT LU177  3.0  I31s STD\"\n",
    "\"CT lung  1.0  I70f  2\"\n",
    "\"CT lung  1.0  I70f low dose\"\n",
    "\"CT lung  1.0  I70f\"\n",
    "\"CT Lung 1.0  I70f  2\"\n",
    "\"CT Lung 1.0  I70f\"\n",
    "\"CT LUNG 1.25 CORR\"\n",
    "\"CT Lung 1.25 Inspi\"\n",
    "\"CT LUNG 1.25 LOW DOSE\"\n",
    "\"CT Lung 1.25 RECON\"\n",
    "\"CT lung HFNIV 1.0  I50f\"\n",
    "\"CT LUNG LOW DOSE 1.25\"\n",
    "\"CT LUNG RECON 1.25\"\n",
    "\"CT LUNG RECON MAN\"\n",
    "\"CT LUNG RECON\"\n",
    "\"CT LUNG\"\n",
    "\"CT Lung1.5  I70f\"\n",
    "\"CT LUNH 1.25 RECON\"\n",
    "\"CT ORL RECON  1.25 STD ASIR40\"\n",
    "\"CT ORL RECON 1\"\n",
    "\"CT ORL RECON 1.25 STD ASIR40\"\n",
    "\"CT ORL RECON 2\"\n",
    "\"CT ORL RECON STD ASIR40\"\n",
    "\"CT ORL RECON\"\n",
    "\"CT ORL STD\"\n",
    "\"CT ORL\"\n",
    "\"CT OS 1.25 RECON\"\n",
    "\"CT OS RECON 1.25 2\"\n",
    "\"CT Pied Drt Cor\"\n",
    "\"CT Pied Drt Sag\"\n",
    "\"CT Pied Drt TRANS\"\n",
    "\"CT PIED RECON 1.25 BONE\"\n",
    "\"CT POUMON RECON\"\n",
    "\"CT poumons NUC  std\"\n",
    "\"CT PS 1.25 RECON\"\n",
    "\"CT PULMONAIRE RESCON 1\"\n",
    "\"CT REC MAND 1.25 STD ASIR40\"\n",
    "\"CT RECON 1.25 BONE\"\n",
    "\"CT RECON 1.25 OS\"\n",
    "\"CT recon 1.25 STD ASIR40\"\n",
    "\"CT Recon 1.25\"\n",
    "\"CT RECON ABDO1.25 STD ASIR40\"\n",
    "\"CT RECON BASSIN OS 1.25\"\n",
    "\"CT RECON BONE + FOV 50\"\n",
    "\"CT RECON BONE 1.25 mm\"\n",
    "\"CT RECON BONE 1.25 STD ASIR40\"\n",
    "\"CT RECON BONE 1.25\"\n",
    "\"CT RECON BONE 1.25*1\"\n",
    "\"CT RECON BONE 1.25/1\"\n",
    "\"CT RECON BONE JAMBES\"\n",
    "\"CT RECON BONE PLUS 1.25\"\n",
    "\"CT Recon bone Tronc  1.0  I70f\"\n",
    "\"CT Recon Bone Tronc 1.0   I70f\"\n",
    "\"CT RECON BONE\"\n",
    "\"CT Recon Brain 2\"\n",
    "\"CT Recon Brain\"\n",
    "\"CT RECON CERVEAU\"\n",
    "\"CT Recon Lung  1.0  I70f  2\"\n",
    "\"CT Recon Lung 1.0  I70f  2\"\n",
    "\"CT Recon Lung 1.25 fov14 nodu\"\n",
    "\"CT RECON LUNG 1.25 STD ASIR40\"\n",
    "\"CT RECON LUNG\"\n",
    "\"CT Recon ORL 1.25\"\n",
    "\"CT RECON ORL 2.5 STD ASIR40\"\n",
    "\"CT RECON ORL STANDARD\"\n",
    "\"CT RECON ORL\"\n",
    "\"CT RECON OS 1.25 OMOPLATE\"\n",
    "\"CT RECON OS 1.25\"\n",
    "\"CT RECON OS BASSIN\"\n",
    "\"CT RECON OS FEMUR D\"\n",
    "\"CT RECON OS WB 5.0 STD ASIR40\"\n",
    "\"CT RECON OS\"\n",
    "\"CT RECON OSSEUX\"\n",
    "\"CT RECON PULM 1.25\"\n",
    "\"CT RECON PULM FOV10\"\n",
    "\"CT RECON PULM\"\n",
    "\"CT Recon Spianl fov16\"\n",
    "\"CT RECON\"\n",
    "\"CT TAP 1.25/0.6 STD ASIR40\"\n",
    "\"CT TAP 2.5 STD ASIR40 RECON2\"\n",
    "\"CT TAP 2.5 STD ASIR40\"\n",
    "\"CT TAP RECON 2.5 STD\"\n",
    "\"CT TAP RECON OS 1.25\"\n",
    "\"CT TETE/THO/ABDO 1.25 BONE+\"\n",
    "\"CT THO 2.5 STD\"\n",
    "\"CT THO Recon OS 1.25\"\n",
    "\"CT THO/ABDO 2.5 STD ASIR40\"\n",
    "\"CT ThoAbd Arteriel  1.0  I70f  3\"\n",
    "\"CT Thorax  1.0  I30 mou\"\n",
    "\"CT Thorax  1.0  I30f\"\n",
    "\"CT Thorax  1.0 50 lung\"\n",
    "\"CT THORAX 3.75 STD ASIR40\"\n",
    "\"CT THORAX ART 2.5\"\n",
    "\"CT Thorax BH 1.0  I30 mou\"\n",
    "\"CT Thorax BH 1.0 50 lung\"\n",
    "\"CT Thorax Lung 1.0  I50f Breathold\"\n",
    "\"CT Thorax Lung 1.25\"\n",
    "\"CT THORAX RECON IODE\"\n",
    "\"CT THORAX RECON PULM 1.25\"\n",
    "\"CT Thorax Std mou 1mm  I30f\"\n",
    "\"CT THX 1.25 ASIR40\"\n",
    "\"CT Tronc  2.0   I30f  3\"\n",
    "\"CT Tronc  2.0  I30f\"\n",
    "\"CT Tronc 1.25 OS ASIR40\"\n",
    "\"CT Tronc 1.25 OSS ASIR40\"\n",
    "\"CT Tronc 1.25 RECON BONE SS40\"\n",
    "\"CT Tronc 1.25 RECON BONE\"\n",
    "\"CT Tronc 1.25 RECON LUNG\"\n",
    "\"CT Tronc 1.25 RECON ORL\"\n",
    "\"CT Tronc 1.25 RETRO ASIR40\"\n",
    "\"CT Tronc 1.25*1 CERVEAU\"\n",
    "\"CT Tronc 2.5 STD ASIR40\"\n",
    "\"CT Tronc 3.75 BONE\"\n",
    "\"CT Tronc 3.75 STD ASIR40\"\n",
    "\"CT TRONC 3.75 STD ASIR40\"\n",
    "\"CT Tronc BONE 1.25 STD ASIR40\"\n",
    "\"CT Tronc low  2.0   I30f  3\"\n",
    "\"CT Tronc low  2.0  I30f\"\n",
    "\"CT Tronc Low dose\"\n",
    "\"CT WB  1.25 ORL STD ASIR40\"\n",
    "\"CT WB  2.0   I30f\"\n",
    "\"CT WB  2.0  B30f\"\n",
    "\"CT WB 1.25 ORL STD ASIR40\"\n",
    "\"CT WB 2.5 STD ASIR40\"\n",
    "\"CT WB 5.0 RECON LUNG\"\n",
    "\"CT WB 5.0 STD ASIR40\"\n",
    "\"CT WB 5.0 STD RECON OS 1.25\"\n",
    "\"CT WB RECON 1.25 BONE\"\n",
    "\"CT WB RECON ORL\"\n",
    "\"CTAC COEUR\"\n",
    "\"CTAC Rest RECON PULM\"\n",
    "\"CTAC Rest\"\n",
    "\"e+1 Pet FDG Tronc 3D MAC\"\n",
    "\"e+1 Pet FDG Tronc MAC EANM\"\n",
    "\"e+2 Pet FDG WB 3D MAC\"\n",
    "\"ECG CaScSeq  3.0  HD_FoV  63%\"\n",
    "\"ECG CaScSeq  3.0  I30f  3  63% fov 50\"\n",
    "\"EPAULE xB  1.0  B70s BONE\"\n",
    "\"ETUDE 3BNC117 [Recon xSPECT - AC ]\"\n",
    "\"Exported MM Oncology Reading\"\n",
    "\"Fdg transax reorient\"\n",
    "\"fFUSION AX AC\"\n",
    "\"fFUSION COR AC\"\n",
    "\"fFUSION SAG AC\"\n",
    "\"FISION AX AC\"\n",
    "\"FIUSION COR  AC\"\n",
    "\"FOIE Y90 MAC SS TOF\"\n",
    "\"FSION COR AC\"\n",
    "\"FSUION COR XBONE\"\n",
    "\"FUCION SAG AC\"\n",
    "\"FUION AX AC\"\n",
    "\"FUION AXIALE\"\n",
    "\"FUISION AX AC\"\n",
    "\"FUISION COR AC\"\n",
    "\"FUISION SAG AC\"\n",
    "\"fuision sagittals\"\n",
    "\"FUISON COR MAA\"\n",
    "\"FUISON PET/MR SAGITTALES\"\n",
    "\"Fuison SAG AC\"\n",
    "\"FUSAION AX AC\"\n",
    "\"FUSAION SAGITALE\"\n",
    "\"FUSIION AX AC\"\n",
    "\"FUSIO AX AC\"\n",
    "\"FUSIO COR AC BASSIN\"\n",
    "\"FUSIO COR AC\"\n",
    "\"FUSIOM AX AC\"\n",
    "\"FUSOIN AX AC\"\n",
    "\"Fuson Cor AC\"\n",
    "\"Fuxion AX AC CHEVILLES\"\n",
    "\"Fuxion AX AC\"\n",
    "\"Fuxion COR AC CHEVILLES\"\n",
    "\"Fuxion COR AC\"\n",
    "\"Fuxion SAG AC CHEVILLES\"\n",
    "\"Fuxion SAG AC\"\n",
    "\"GENOUX  LD 1.0  I80s BONE\"\n",
    "\"Genxproth_IRACRR_Trans vue post\"\n",
    "\"GGL Ax G Sentinelle\"\n",
    "\"Hanches xB 1.0  B70s BONE\"\n",
    "\"IMAGES CLE\"\n",
    "\"KEY IMAGES\"\n",
    "\"LEUCO [Recon xSPECT - AC ]\"\n",
    "\"Lu-177 [Recon xSPECT - AC ]\"\n",
    "\"LUNG RET\"\n",
    "\"MAA FOIE [Recon Flash3D - AC ]\"\n",
    "\"MAA FOIE [Recon xSPECT - AC ] FUSED\"\n",
    "\"MAA FOIE [Recon xSPECT - AC ]\"\n",
    "\"MAA FOIE FUSED\"\n",
    "\"MAA FOIE FUSION HD\"\n",
    "\"MFSC FUSION AX AC\"\n",
    "\"MFSC FUSION AX XBONE\"\n",
    "\"MFSC FUSION COR AC\"\n",
    "\"MFSC FUSION COR XBONE\"\n",
    "\"MFSC FUSION SAG AC\"\n",
    "\"MFSC FUSION SAG XBONE\"\n",
    "\"MIBG [Recon xSPECT - AC ]\"\n",
    "\"MixedPatient\"\n",
    "\"Os_1Phase [Recon Flash3D - NoAC ]\"\n",
    "\"Os_1Phase [Recon xSPECT - AC ]\"\n",
    "\"Os_1Phase fusi [Recon xSPECT - AC ]\"\n",
    "\"Os_3Phases [Recon Flash3D - NoAC - NoAC ]\"\n",
    "\"Os_3Phases [Recon xSPECT - AC ]\"\n",
    "\"Perf axiale\"\n",
    "\"PerfTransaxial\"\n",
    "\"Perfu coronal\"\n",
    "\"PerfusionTransaxial\"\n",
    "\"PET Cardiac FUSION GLG\"\n",
    "\"PET Cardiac Static\"\n",
    "\"PET CardioFreeze dual 8 Gates\"\n",
    "\"Pet Centrage EANM\"\n",
    "\"PET CT FDG Tronc 256 matrix\"\n",
    "\"PET FDG Att Tronc 3h\"\n",
    "\"Pet FDG Centrage MAC\"\n",
    "\"Pet FDG Cerv Dyn 20min MAC\"\n",
    "\"Pet FDG Cerv fusion 10min\"\n",
    "\"Pet FDG Cerv fusion 11-20min\"\n",
    "\"Pet FDG Coeur Stat 10min MAC\"\n",
    "\"Pet FDG EANM Tronc-ORL MAC\"\n",
    "\"PET FDG HFNIV Flow\"\n",
    "\"Pet FDG MAC Tronc-ORL 3D coro\"\n",
    "\"Pet FDG MAC Tronc-ORL 3D sag\"\n",
    "\"Pet FDG MAC Tronc-ORL 3D\"\n",
    "\"PET FDG Tho HFNIV Flow\"\n",
    "\"PET FDG Tho HFNIV NAC Flow\"\n",
    "\"PET FDG Thorax  Hold AC LM 256\"\n",
    "\"PET FDG Thorax  Hold AC LM jop\"\n",
    "\"PET FDG Thorax  Hold AC LM\"\n",
    "\"Pet FDG Thorax MAC\"\n",
    "\"PET FDG Tronc 220 matrix\"\n",
    "\"PET FDG Tronc 256 matrix\"\n",
    "\"Pet FDG Tronc 3D MAC FUSION AXIALE\"\n",
    "\"Pet FDG Tronc 3D MAC fusion pet mr\"\n",
    "\"Pet FDG Tronc 3D MAC ok\"\n",
    "\"Pet FDG Tronc 3D MAC\"\n",
    "\"PET FDG Tronc AC Flow 220 matrix\"\n",
    "\"PET FDG Tronc AC Flow 256 matrix\"\n",
    "\"PET FDG Tronc AC Flow 3min\"\n",
    "\"PET FDG Tronc AC Flow 4min\"\n",
    "\"PET FDG Tronc EANM\"\n",
    "\"PET FDG Tronc Flow 220 matrix\"\n",
    "\"PET FDG Tronc Flow 256 matrix\"\n",
    "\"PET FDG Tronc Flow ceCT axial\"\n",
    "\"PET FDG Tronc Flow EANM\"\n",
    "\"PET FDG Tronc Flow recon corrige\"\n",
    "\"PET FDG Tronc Flow\"\n",
    "\"PET FDG Tronc MAC 1min30\"\n",
    "\"PET FDG Tronc MAC 2min\"\n",
    "\"Pet FDG Tronc MAC EANM\"\n",
    "\"Pet FDG Tronc MAC\"\n",
    "\"PET FDG Tronc OncoFreeze 256 matrix\"\n",
    "\"PET FDG Tronc OncoFreeze\"\n",
    "\"PET FDG Tronc\"\n",
    "\"Pet FDG WB 3D MAC\"\n",
    "\"Pet FDG WB EANM\"\n",
    "\"PET FDG WB Flow EANM\"\n",
    "\"PET FDG WB Flow\"\n",
    "\"PET FET Cerveau 40-50 min\"\n",
    "\"Pet FET Cerveau 40-50min MAC\"\n",
    "\"Pet FET Cerveau MAC 50min\"\n",
    "\"Pet FET fusion MAC 50min\"\n",
    "\"PET Folate Tronc Flow\"\n",
    "\"PET FUSI CFsingle 8 Gates\"\n",
    "\"Pet GA68 DOTA Coeur 10min MAC\"\n",
    "\"PET Ga68 DOTA Thorax Flow\"\n",
    "\"Pet GA68 DOTA Thorax MAC\"\n",
    "\"PET Ga68 DOTA Tronc EANM\"\n",
    "\"PET Ga68 Dota Tronc Flow\"\n",
    "\"Pet GA68 DOTA Tronc MAC\"\n",
    "\"PET Ga68 Dotatate Thorax EANM\"\n",
    "\"PET Ga68 Dotatate Thorax\"\n",
    "\"PET Ga68 Dotatate Tronc\"\n",
    "\"PET Ga68 Dotatate WB Flow 220 matrix\"\n",
    "\"PET Ga68 DotatateTronc Flow EANM\"\n",
    "\"PET Ga68 Dotatoc Tronc Flow\"\n",
    "\"PET Ga68 DOTATOC Tronc Flow\"\n",
    "\"Pet Ga68 EANM DOTA Tronc\"\n",
    "\"Pet Ga68 MAC DOTA Tronc\"\n",
    "\"Pet Ga68 MAC PSMA Tronc 2.128\"\n",
    "\"Pet Ga68 MAC PSMA Tronc 3MIN\"\n",
    "\"Pet Ga68 MAC PSMA Tronc\"\n",
    "\"PET Ga68 PSMA Tronc AC\"\n",
    "\"PET Ga68 PSMA Tronc Flow\"\n",
    "\"PET Ga68 PSMA Tronc\"\n",
    "\"PET Ga68 RGD Nodaga Tronc\"\n",
    "\"PET Gallium Dotatate Tronc 4i5s5f 220\"\n",
    "\"Pet GDOT Coeur Stat 10min MAC fusio\"\n",
    "\"Pet GTATE  Thorax MAC fusion rate\"\n",
    "\"Pet GTATE  Thorax MAC\"\n",
    "\"Pet GTATE Coeur 10min MAC\"\n",
    "\"Pet RM\"\n",
    "\"Pet Tronc 3D MAC\"\n",
    "\"PET TRONC 3D MAC\"\n",
    "\"PET Tronc Flow\"\n",
    "\"PET Tronc ORL Flow sag\"\n",
    "\"PET Tronc ORL Flow\"\n",
    "\"PET Tronc ORLFlow 220 matrix\"\n",
    "\"Pet Tronc-ORL 3D MAC\"\n",
    "\"PET WB 220 matrix\"\n",
    "\"PET WB 30sec_bed\"\n",
    "\"PET WB Dynamic\"\n",
    "\"PET WB Flow 220 matrix\"\n",
    "\"PET WB Flow 256 matrix\"\n",
    "\"PET WB Flow EANM\"\n",
    "\"PET WB Flow\"\n",
    "\"Pet WB Ga68 Dota MAC\"\n",
    "\"PET WB OncoFreeze\"\n",
    "\"PET Y90 30min TOF 2i5s Absolut\"\n",
    "\"PET Y90 Foie HD 30min\"\n",
    "\"PET Y90 Foie NAC\"\n",
    "\"PET Y90 Foie Oncofreeze 20min\"\n",
    "\"PET Y90 Fused\"\n",
    "\"pet\"\n",
    "\"PET/MR CORONALES\"\n",
    "\"PET/MR SAGITTALES\"\n",
    "\"Pet-CT  FDG Tronc MAC\"\n",
    "\"PET-CT FDG Tronc MAC\"\n",
    "\"Pet-CT Tronc 3D MAC\"\n",
    "\"Pet-CT Tronc EANM\"\n",
    "\"Pet-CT Tronc Ga68Dota EANM\"\n",
    "\"Pet-CT Tronc Ga68Dota MAC\"\n",
    "\"Pet-CT WB 3D MAC\"\n",
    "\"Pet-CT WB EANM\"\n",
    "\"Petites articulat  1.0  I80s BONE\"\n",
    "\"poumons\"\n",
    "\"PreMonitoring  10.0  B30s\"\n",
    "\"Quant. Perf Analysis: Posterior\"\n",
    "\"Range-AC  CT Thorax  2.0  HD_FoV   iMAR-Tra-<ALPHA Range>\"\n",
    "\"Range-AC  CT TRONC  2.0  HD_FoV   iMAR-Tra-<ALPHA Range>\"\n",
    "\"Range-CT Tronc  2.0  I30f   iMAR-Tra-<ALPHA Range>\"\n",
    "\"REC OS DORSO LOMB\"\n",
    "\"RECON 1.25 THORAX\"\n",
    "\"recon 1.25 tissu nou\"\n",
    "\"recon 1.25\"\n",
    "\"RECON 1.25\"\n",
    "\"recon 1.25mm\"\n",
    "\"RECON 1.25mm\"\n",
    "\"RECON 1.25ORL\"\n",
    "\"recon 2.5mm\"\n",
    "\"RECON ABDO BONE 1.25/1\"\n",
    "\"RECON BASSIN 1.25\"\n",
    "\"Recon Bone 1,25mm\"\n",
    "\"RECON BONE 1.25 mm\"\n",
    "\"Recon bone 1.25\"\n",
    "\"RECON BONE CRANE\"\n",
    "\"RECON BONE CT 1.25\"\n",
    "\"recon bone tibia\"\n",
    "\"RECON BONE\"\n",
    "\"RECON CENTREE LUNG\"\n",
    "\"RECON CT 1.25*1\"\n",
    "\"RECON CT BONE 1.25\"\n",
    "\"RECON CT BONE 1.25*1\"\n",
    "\"RECON CT BONE 1.25/1\"\n",
    "\"RECON CT LUNG\"\n",
    "\"RECON CT OS\"\n",
    "\"RECON FOV 40\"\n",
    "\"recon lung\"\n",
    "\"Recon Lung\"\n",
    "\"RECON LUNG\"\n",
    "\"RECON ORL 1.25\"\n",
    "\"RECON ORL 1.25*1\"\n",
    "\"RECON ORL 1.25MM\"\n",
    "\"RECON ORL\"\n",
    "\"RECON OS 1.25\"\n",
    "\"RECON OS 1.25MM\"\n",
    "\"RECON OS FINE CRANE\"\n",
    "\"RECON OS\"\n",
    "\"recon oss\"\n",
    "\"recon osseuse\"\n",
    "\"RECON POUMON\"\n",
    "\"RECON POUMONS\"\n",
    "\"RECON PROTHESE 1.25 CORRE\"\n",
    "\"recon pulm\"\n",
    "\"Recon Pulm\"\n",
    "\"RECON PULM\"\n",
    "\"RECON PULMOAIRE 1.25\"\n",
    "\"RECON PULMONAIRE 1.25\"\n",
    "\"recon pulmonaire\"\n",
    "\"RECON STD CRANE\"\n",
    "\"RECON THORAX\"\n",
    "\"RECON TRONC BONE 1.25*1\"\n",
    "\"recon\"\n",
    "\"RECON\"\n",
    "\"recons os\"\n",
    "\"RECONST PULMO\"\n",
    "\"RECONT CT BONE 1.25*1\"\n",
    "\"Reformatted\"\n",
    "\"Rest  PET CardioFreeze dual 8 Gates\"\n",
    "\"Reults\"\n",
    "\"Rois01\"\n",
    "\"Rois2\"\n",
    "\"SAG AC\"\n",
    "\"SAG MN\"\n",
    "\"SAG\"\n",
    "\"SAGITAL FD MAA\"\n",
    "\"SAGITAL MAA\"\n",
    "\"SAGITAL\"\n",
    "\"SAGITALS MAA\"\n",
    "\"sagitals\"\n",
    "\"SAGITTAL\"\n",
    "\"SAGITTALS maa\"\n",
    "\"SAVED IMAGES\"\n",
    "\"SCINTI OSSEUSE 1 PHASE [Recon xSPECT - AC ]\"\n",
    "\"SCINTI OSSEUSE 1 PHASE [Recon xSPECT ABDO - AC ]\"\n",
    "\"SCINTI OSSEUSE 3 PHASES [Recon xSPECT - AC ]\"\n",
    "\"SCINTI PULM PERFUSEE [Recon xSPECT\"\n",
    "\"shunt 1.30%\"\n",
    "\"SIG SERIES\"\n",
    "\"SIRT MAA [Recon xSPECT - AC ]\"\n",
    "\"SIRT MAA SAG\"\n",
    "\"Spect  Col Dor_IRAC_Transaxials\"\n",
    "\"SPECT [Recon xSPECT - AC ]\"\n",
    "\"SPECT ABDO_EM_IRAC_Transaxials\"\n",
    "\"SPECT CT PERF PULM AX\"\n",
    "\"SPECT CT PERF PULM CORO\"\n",
    "\"SPECT CT PERF PULM SAG\"\n",
    "\"Spect ct sag\"\n",
    "\"Spect hanchepro_IRACRR_Transaxials\"\n",
    "\"SPECT HANCHES [Recon xSPECT - AC ]\"\n",
    "\"SPECT MAA Foie FUSION AXIAL\"\n",
    "\"SPECT MAA Foie FUSION CORONAL\"\n",
    "\"SPECT MAA Foie_EM_IRNC_Transaxials\"\n",
    "\"SPECT VENT TC_IRNC_Transaxials\"\n",
    "\"SPECT x1 Lu-177_EM_IRAC_Transaxials\"\n",
    "\"SPECT x1 Lu-177_EM_IRNC_Transaxials\"\n",
    "\"SPECT-CT PERF TC_EM_IRAC_Transaxials\"\n",
    "\"SPECT-CT PERF TC_EM_IRNC_Transaxials\"\n",
    "\"State - +SRest Static 6min Recon MAC\"\n",
    "\"State - +SStress Static 6min Recon MAC\"\n",
    "\"Static\"\n",
    "\"TAP veineux  3.0  I30f  3\"\n",
    "\"ThoAbd Arteriel lung  1.0  I70f  3\"\n",
    "\"Thorax LD  1.5  I31s Std\"\n",
    "\"Thorax LD  1.5  I80s Lung\"\n",
    "\"TRANS MN\"\n",
    "\"transaxial perf\"\n",
    "\"transaxial PERF\"\n",
    "\"transaxial vent\"\n",
    "\"transaxial\"\n",
    "\"TRANSAXIAL\"\n",
    "\"TRANSAXIALS maa\"\n",
    "\"transaxials\"\n",
    "\"TRANSAXIALS\"\n",
    "\"TRECON OS\"\n",
    "\"TTT poumon\"\n",
    "\"Ventil coronal\"\n",
    "\"Ventil transaxial\"\n",
    "\"VENTILATION TC [Recon xSPECT - AC ]\"\n",
    "\"ventilation transax\"\n",
    "\"VENTILATION\"\n",
    "\"VentilTransaxial\"\n",
    "\"Y90 Microspheres [Recon Flash3D - AC ]\"\n",
    "\"Y90 Microspheres [Recon Flash3D -\"\n",
    "\"Y90 Microspheres Axial Fused\"\n",
    "\"Y90 Microspheres Fusion Axial\"\n",
    "\"Y90 SPECT/ceCT\"\n",
    "\"Y90 SPECT/CT AXIAL\"\n",
    "\"Y90 SPECT/CT\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20160101'\n",
    "config['main']['end_date'] = '20191227'\n",
    "\n",
    "_, df = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = descr_list.split('\"\\n\"')\n",
    "df['in'] = df['Series Description'].isin(d)\n",
    "c = df.groupby('Series Description').sum()\n",
    "c[c['in'] > 0].sort_values('in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set([print('^' + pat.lower() + '$') for pat in (set(d)).difference(set(df['Series Description']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(set(df['Series Description'])).difference(set(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Schedule visuals #63\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191215'\n",
    "config['main']['end_date'] = '20191220'\n",
    "machine = 'PET GE'\n",
    "config['draw']['debug_single_machine'] = machine\n",
    "config['main']['mode'] = 'single'\n",
    "config['draw']['debug_save_as_image'] = 'True'\n",
    "config['draw']['dpi'] = '200'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Number of slots per day #48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191215'\n",
    "config['main']['end_date'] = '20191220'\n",
    "machine = 'PET GE'\n",
    "config['draw']['debug_single_machine'] = machine\n",
    "config['main']['mode'] = 'single'\n",
    "config['draw']['debug_save_as_image'] = 'True'\n",
    "config['draw']['dpi'] = '100'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_series_for_day = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('Description == \"VENT PERF.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vent_perf = df.reset_index().query('Description == \"VENT PERF.\"').groupby(['Patient ID', 'Date', 'Description', 'Machine']).agg({\n",
    "    'SUID': lambda x: x.iloc[0], \n",
    "    'Patient ID': lambda x: x.iloc[0], \n",
    "    'Date': lambda x: x.iloc[0], \n",
    "    'Description': lambda x: x.iloc[0], \n",
    "    'Machine': lambda x: x.iloc[0], \n",
    "    'Study Description': lambda x: ' / '.join(set(x)), \n",
    "    'Modality': lambda x: '/'.join(set(x)).replace('CT/CT', 'CT').replace('NM/NM', 'NM'), \n",
    "    'Machine Group': lambda x: x.iloc[1], \n",
    "    'Start Time': 'min',\n",
    "    'End Time': 'max',\n",
    "    'Start Time Prep': 'min',\n",
    "    'End Time Prep': 'max'}).set_index('SUID')\n",
    "df = df[df['Description'] != 'VENT PERF.']\n",
    "df = pd.concat([df, df_vent_perf], sort=False).sort_values(['Start Time', 'Machine Group', 'SUID'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date_weekday'] = list(pd.to_datetime(df['Date']).apply(lambda x: x.weekday()))\n",
    "n_slots_per_day = pd.Series(config['draw']['n_study_per_day_' + machine.lower().replace(' ', '')].split(',')).astype(int)\n",
    "df['Date_weekday'].apply(lambda x: n_slots_per_day[x]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 25): display(df.groupby(['Machine', 'Date_weekday'])['Date'].count())\n",
    "with pd.option_context('display.max_rows', 25): display(df.groupby(['Machine', 'Date'])['Date'].count().groupby(['Machine']).count())\n",
    "with pd.option_context('display.max_rows', 25): display(df.groupby(['Machine', 'Date_weekday'])['Date'].count() / df.groupby(['Machine', 'Date'])['Date'].count().groupby(['Machine']).count() * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Split OS3PHASES into two parts #27\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20190101'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['draw']['debug_single_machine'] = 'Discovery 670 NM'\n",
    "config['main']['mode'] = 'single'\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)\n",
    "df_studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_times = pd.to_datetime(df_studies['Start Time'], format='%H%M%S')\n",
    "end_times = pd.to_datetime(df_studies['End Time'], format='%H%M%S')\n",
    "df_studies['duration'] = end_times - start_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_studies['i_take'] = list(df_studies.reset_index()['SUID'].apply(lambda x: x.split('_')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies.groupby(['Description', 'i_take']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies.groupby(['Description', 'i_take'])['duration'].agg(duration=pd.Series.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191209'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['draw']['debug_single_machine'] = 'Discovery 670 NM'\n",
    "config['main']['mode'] = 'single'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Add grid lines for each hour behind the schedule plot #35\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20190902'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['main']['mode'] = 'single'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for weird overlappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191202'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['main']['mode'] = 'single'\n",
    "config['draw']['debug_single_machine'] = 'Discovery 670 NM'\n",
    "config['draw']['debug_schedule_show_IPP_string'] = 'False'\n",
    "config['draw']['debug_save_as_image'] = 'False'\n",
    "config['draw']['dpi'] = '150'\n",
    "config['extract']['debug_force_extract_studies'] = 'False'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from extract_data import mark_retakes\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)\n",
    "df = df_series.query('`Machine Group` == \"Discovery 670 NM\" and Date == \"20191212\"')\n",
    "with pd.option_context('display.max_colwidth', -1): display(df[['Patient ID', 'Modality', 'Start Time', 'End Time', 'Series Description', 'ImageType', 'Study Instance UID', 'i_take']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_studies.query('Machine == \"Discovery 670 NM\" and Date == \"20191212\" and `Patient ID` == \"2948602\"')\n",
    "display(df[['Modality', 'Start Time', 'End Time', 'Description']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Calculate inter study time #49\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191209'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['main']['mode'] = 'single'\n",
    "df_all, _ = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query('Machine == \"Discovery 670 NM\" and Date >= \"20191209\" and Date <= \"20191213\"').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FMT = '%H%M%S'\n",
    "df['Start Time'] = pd.to_datetime(df['Start Time'], format=FMT)\n",
    "df['End Time'] = pd.to_datetime(df['End Time'], format=FMT)\n",
    "df['Start Time Prep'] = pd.to_datetime(df['Start Time Prep'], format=FMT)\n",
    "df['End Time Prep'] = pd.to_datetime(df['End Time Prep'], format=FMT)\n",
    "# compare the start time of a row with the end time of the previous row\n",
    "df['time_to_prev'] = df['End Time'].shift() - df['Start Time']\n",
    "df.loc[df['time_to_prev'] < timedelta(0), 'time_to_prev'] *= -1\n",
    "df['time_to_prev_prep'] = df['End Time Prep'].shift() - df['Start Time Prep']\n",
    "df.loc[df['time_to_prev_prep'] < timedelta(0), 'time_to_prev_prep'] *= -1\n",
    "# compare the end time of a row with the start time of the next row\n",
    "df['time_to_next'] = df['Start Time'].shift(-1) - df['End Time']\n",
    "df.loc[df['time_to_next'] < timedelta(0), 'time_to_next'] *= -1\n",
    "df['time_to_next_prep'] = df['Start Time Prep'].shift(-1) - df['End Time Prep']\n",
    "df.loc[df['time_to_next_prep'] < timedelta(0), 'time_to_next_prep'] *= -1\n",
    "# get the fully contained rows\n",
    "df['fully_contained'] = (df['End Time'] < df['End Time'].shift()) & (df['Start Time'] > df['Start Time'].shift()) & (df['Date'].shift() == df['Date']) & (df['Date'].shift(-1) == df['Date'])\n",
    "# make sure that we only keep values where the dates are identical\n",
    "df.loc[df['Date'] != df['Date'].shift(), 'time_to_prev'] = pd.NaT\n",
    "df.loc[df['Date'] != df['Date'].shift(), 'time_to_prev_prep'] = pd.NaT\n",
    "df.loc[df['Date'] != df['Date'].shift(-1), 'time_to_next'] = pd.NaT\n",
    "df.loc[df['Date'] != df['Date'].shift(-1), 'time_to_next_prep'] = pd.NaT\n",
    "# get the average times\n",
    "average_times = df.groupby(['Machine', 'Description'])[['time_to_prev', 'time_to_prev_prep', 'time_to_next', 'time_to_next_prep']].agg(pd.Series.mean)\n",
    "average_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Add preparation times around study blocks #21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191125'\n",
    "config['main']['end_date'] = '20191125'\n",
    "config['main']['mode'] = 'single'\n",
    "df_all, _ = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query('Machine == \"PET GE\" and Date == \"20191125\"').copy()\n",
    "display(df)\n",
    "df = add_preparation_times(config, df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191125'\n",
    "config['main']['end_date'] = '20191125'\n",
    "config['main']['mode'] = 'single'\n",
    "config['draw']['debug_single_machine'] = 'PET GE'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Add bookmarks to PDF #53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "config = load_config()\n",
    "pdf_output_path, bookmarks = create_report(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the handles of the created bookmarks\n",
    "bookmark_handles = {}\n",
    "\n",
    "# read in the input file and copy it to output while adding bookmarks\n",
    "with open(pdf_output_path, 'rb') as input_file:\n",
    "    # define the input and output objects\n",
    "    reader = PdfFileReader(input_file)\n",
    "    writer = PdfFileWriter()\n",
    "    # copy meta data\n",
    "    metadata = reader.getDocumentInfo()\n",
    "    writer.addMetadata(metadata)\n",
    "    # go through the bookmarks\n",
    "    i_page, parent_bookmark_handle = 0, None\n",
    "    for bookmark in bookmarks:\n",
    "        # if we encounter a page we did not copy yet, add it\n",
    "        if i_page == bookmark.page:\n",
    "            writer.addPage(reader.getPage(i_page))\n",
    "            i_page += 1\n",
    "            print(i_page)\n",
    "        # if the bookmark has the previous bookmark as a parent\n",
    "        if bookmark.parent is not None and bookmark.parent in bookmark_handles.keys():\n",
    "            bookmark_handles[bookmark.title] = \\\n",
    "                writer.addBookmark(bookmark.title, bookmark.page, bookmark_handles[bookmark.parent])\n",
    "        else:\n",
    "            bookmark_handles[bookmark.title] = writer.addBookmark(bookmark.title, bookmark.page)\n",
    "\n",
    "# write out the file\n",
    "with open(pdf_output_path.replace('.pdf', '2.pdf'), 'wb') as out:\n",
    "    writer.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Make report send email #54\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "config = load_config()\n",
    "now_str = dt.now().strftime('%Y-%m-%d_%Hh%Mm%Ss')\n",
    "pdf_output_path = '{}/report_{}.pdf'.format(config['path']['output_dir'], now_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relevant part of the config\n",
    "email = config['email']\n",
    "\n",
    "# get the body and the subject of the mail\n",
    "body = email['body']\n",
    "subject = email['subject']\n",
    "\n",
    "# split the path into pieces\n",
    "report_file_name = pdf_output_path.split('/')[-1]\n",
    "report_folder_path = pdf_output_path.split('/')[:-1]\n",
    "\n",
    "# replace the relevant parts \n",
    "body = body.replace('{__REPORT_PATH__}', pdf_output_path)\n",
    "body = body.replace('{__REPORT_FOLDER_PATH__}', pdf_output_path)\n",
    "body = body.replace('{__REPORT_FILE_NAME__}', pdf_output_path)\n",
    "\n",
    "\n",
    "print(subject)\n",
    "print(body)\n",
    "return\n",
    "# create the body and populate it with headers\n",
    "msg = MIMEText(body, 'html')\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = email['sender_name'] + '<' + email['sender_email'] + '>'\n",
    "msg['To'] = email['recipients_email']\n",
    "\n",
    "# create the connection to the server and send the mail\n",
    "s = smtplib.SMTP(email['smtp_server'])\n",
    "s.sendmail(email['sender_email'], email['recipients_email'].split(','), msg.as_string())\n",
    "# close the sessions\n",
    "s.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "output = PdfFileWriter()\n",
    "input1 = PdfFileReader(open('C:/TEMP/SchedVisu/outputs/report_2019-12-18_10h47m03s.pdf', 'rb'))\n",
    "output.addPage(input1.getPage(0))\n",
    "output.addPage(input1.getPage(1))\n",
    "output.addPage(input1.getPage(2))\n",
    "parent = output.addBookmark('TestParent', 0) # add parent bookmark\n",
    "output.addBookmark('TestChild1', 0, parent) # add child bookmark\n",
    "output.addBookmark('TestChild2', 1, parent) # add child bookmark\n",
    "output.addBookmark('TestChild3', 2, parent) # add child bookmark\n",
    "with open('C:/TEMP/SchedVisu/outputs/test.pdf', 'wb') as out:\n",
    "    output.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tests for comparing to VENUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from create_report import _set_schedule_y_lims\n",
    "from extract_data import load_transform_and_save_data_from_files\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.colors as mc\n",
    "import colorsys\n",
    "\n",
    "config = load_config()\n",
    "config['main']['start_date'] = '20191205'\n",
    "config['main']['end_date'] = '20191205'\n",
    "\n",
    "# either go through all available machines, or use the list specified by the config\n",
    "machines_list = sorted(list(set([machine for machine in config['machines'].keys() if 'NoCT' not in machine])))\n",
    "if config['draw']['debug_single_machine'] != '*':\n",
    "    machines_list = config['draw']['debug_single_machine'].split(',')\n",
    "\n",
    "# get the starting and ending dates, and the days range from the config\n",
    "start_date, end_date, days_range = main.get_day_range(config)\n",
    "\n",
    "# create a matplotlib figure with the right aspect ratio\n",
    "fig = plt.figure(figsize=[8.27, 11.69])\n",
    "    \n",
    "# create the new axes\n",
    "sched_ax = fig.add_axes([0.06, 0.42, 0.80, 0.39], anchor='NE')\n",
    "sched_ax.invert_yaxis()\n",
    "\n",
    "# create the ticks and labels, with a reduced frequency\n",
    "_, _, days_range_xticks = main.get_day_range(config, reduce_freq=True)\n",
    "days_xticks, days_xtick_labels = [], []\n",
    "\n",
    "# plot each day\n",
    "i_day = 0\n",
    "n_days_to_show = len(days_range)\n",
    "\n",
    "df, _ = load_transform_and_save_data_from_files(config)\n",
    "# exclude some machines and do some grouping up\n",
    "df['Machine'] = df['Machine Group'].str.replace('NoCT', '')\n",
    "df = df[df['Machine'] != 'mixed cases']\n",
    "    \n",
    "# go through each machine\n",
    "for machine in machines_list:\n",
    "\n",
    "    # get the data for the current day and machine\n",
    "    df_day = df.query('Machine == \"{}\"'.format(machine))\n",
    "\n",
    "    # go through each study found for this machine\n",
    "    for i_study in range(len(df_day)):\n",
    "        study = df_day.iloc[i_study, :]\n",
    "\n",
    "        # get the start time, end time and duration as hours with decimals\n",
    "        start = pd.to_datetime(study['Start Time'], format='%H%M%S')\n",
    "        end = pd.to_datetime(study['End Time'], format='%H%M%S')\n",
    "        start_hour = start.hour + start.minute / 60 + start.second / 3600\n",
    "        end_hour = end.hour + end.minute / 60 + end.second / 3600\n",
    "        duration_hours = end_hour - start_hour\n",
    "\n",
    "        # if the duration is negative\n",
    "        if duration_hours <= 0: continue\n",
    "\n",
    "        # get the coordinates where the rounded rectangle for this study should be plotted\n",
    "        box_w = config['draw'].getfloat('study_box_w')\n",
    "        x_shift = config['draw'].getfloat('study_x_shift')\n",
    "        x = i_day - (box_w * 0.5) + (x_shift * (-1 if (i_study % 2 == 0) else 1))\n",
    "        y, w, h = start_hour, box_w, duration_hours\n",
    "\n",
    "        # check if we have an overlap issue\n",
    "        if i_study > 0:\n",
    "            end_prev = pd.to_datetime(df_day.iloc[i_study - 1, :]['End Time'], format='%H%M%S')\n",
    "            end_prev_hour = end_prev.hour + end_prev.minute / 60 + end_prev.second / 3600\n",
    "            \n",
    "            # check how long the gap was with previous study\n",
    "            gap_duration_hour = start_hour - end_prev_hour\n",
    "            gap_threshold = config['draw'].getfloat('gap_dur_minutes_' + machine.lower().replace(' ', ''))\n",
    "            if gap_duration_hour * 60 >= gap_threshold:\n",
    "                # plot a black line to show gaps\n",
    "                plt.plot([i_day, i_day], [start_hour - 0.15, end_prev_hour + 0.15],\n",
    "                    color='black', linestyle='dashed', linewidth=2)\n",
    "\n",
    "        # define colors\n",
    "        descr_list = list(config['description_' + machine.lower().replace(' ', '')].keys()) + ['OTHER']\n",
    "        colors = colors = config['draw']['colors'].split(',')\n",
    "        i_descr = descr_list.index(study['Description'])\n",
    "\n",
    "        # check if the current study is a retake\n",
    "        try:\n",
    "            i_take = int(study.name.split('_')[-1])\n",
    "        except ValueError:\n",
    "            logging.warning('Problem with study ...{} on {}: got a weird retake number: \"{}\"'\n",
    "                .format('.'.join(study.name.split('.')[-2:]), machine,  study.name))\n",
    "            i_take = 1\n",
    "\n",
    "        hatch, edge_color = '', 'black'\n",
    "        if i_take != 1:\n",
    "            logging.debug(study.name + ' is a retake (reprise)')\n",
    "            hatch = '/'\n",
    "            edge_color = 'red'\n",
    "            sibling_studies_patches = [\n",
    "                    p for p in sched_ax.patches\n",
    "                    if p._label.split('_')[0] == study.name.split('_')[0]\n",
    "                ]\n",
    "            for p in sibling_studies_patches:\n",
    "                p.set_hatch('\\\\')\n",
    "                p.set_edgecolor('red')\n",
    "\n",
    "        # if we are displaying more than ~4 months, the inside of the blocks is not visible anymore.\n",
    "        #   Therefore, we need to use the edge to show the colors\n",
    "        if len(days_range) > 95:\n",
    "            edge_color = colors[i_descr]\n",
    "            hatch = ''\n",
    "\n",
    "        # create the shape and plot it\n",
    "        rounded_rect = FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=-0.0040,rounding_size=0.155\",\n",
    "            fc=colors[i_descr], ec=edge_color, mutation_aspect=0.3, hatch=hatch, label=study.name)\n",
    "        sched_ax.add_patch(rounded_rect)\n",
    "\n",
    "        # DEBUG show information string\n",
    "        if config['draw'].getboolean('debug_schedule_show_IPP_string'):\n",
    "            plt.text(x + w * 0.1, y + 0.9 * h, study['Patient ID'], fontsize=8)\n",
    "\n",
    "    i_day += 1\n",
    "    \n",
    "plt.xlim([-0.5, i_day - 0.5])\n",
    "plt.xticks(range(i_day), machines_list)\n",
    "# set the y limits\n",
    "_set_schedule_y_lims(config, df)\n",
    "\n",
    "fig.savefig('schedvisu_20191205.png', orientation='portrait', papertype='a4', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for CTDIvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_all = pd.read_pickle('C:/TEMP/SchedVisu/data/series.pkl')\n",
    "df_all2019 = df_all.query('Date > \"20191101\"').copy()\n",
    "df_noctdi = df_all2019[df_all2019['CTDIvol'].isnull()].copy()\n",
    "df = df_all2019[~df_all2019['CTDIvol'].isnull()].copy()\n",
    "print(f'Found {len(df)} series with CTDIvol information out of {len(df_all2019)} series ({len(df_noctdi)} have no information)')\n",
    "df = df.rename(columns={'CTDIvol':'CTDIvol_ori'})\n",
    "df['CTDIvol'] = df['CTDIvol_ori'].apply(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_data import create_description_consensus\n",
    "df['Study Description'] = df['Study Description'].replace(np.nan, '')\n",
    "df = create_description_consensus(config, df)\n",
    "df_all2019['Study Description'] = df_all2019['Study Description'].replace(np.nan, '')\n",
    "df_all2019 = create_description_consensus(config, df_all2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df[['Patient ID', 'Date', 'Modality', 'Start Time', 'End Time', 'Description', 'Study Description', 'Series Description', 'Machine', 'CTDIvol', 'CTDIvol_ori', 'ImageType']].to_excel('CTDI_vol.xls')\n",
    "df_all2019[['Patient ID', 'Date', 'Modality', 'Start Time', 'End Time', 'Description', 'Study Description', 'Series Description', 'Machine', 'CTDIvol', 'ImageType']].to_excel('CTDI_vol_all.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[['CTDIvol', 'Description', 'Patient ID']].groupby(['CTDIvol', 'Description'])['Patient ID'].count().sort_values(ascending=False).to_csv('ctdivol_count_description.csv', header=True)\n",
    "df[['CTDIvol', 'Series Description', 'Patient ID']].groupby(['CTDIvol', 'Series Description'])['Patient ID'].count().sort_values(ascending=False).to_csv('ctdivol_count_series_description.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('max_rows', -1): display(df.groupby(['CTDIvol', 'Series Description'])['Patient ID'].count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('max_rows', -1): display(df.groupby(['CTDIvol', 'Description'])['Patient ID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for other exam types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/TEMP/SchedVisu/data/studies.pkl')\n",
    "descr = df.query('Description == \"OTHER\" and Machine == \"Millennium\"').groupby('Study Description')['Date'].count().sort_values()\n",
    "descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for creating a multi-page PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Report Type classification wrong #36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191125'\n",
    "config['main']['end_date'] = '20191219'\n",
    "start_date, end_date, _ = main.get_day_range(config)\n",
    "print((end_date - start_date).days)\n",
    "print(end_date - start_date)\n",
    "get_report_type(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Dose report #5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series = pd.read_pickle('C:/TEMP/SchedVisu_data/series.pkl')\n",
    "print(df_series.columns)\n",
    "df_ctdivol = df_series[(~df_series['CTDIvol'].isnull()) | (~df_series['CTDIvol_start'].isnull())][['Date', 'CTDIvol_start', 'Series Description', 'Study Description', 'Protocol Name', 'Modality', 'Patient ID', 'ImageType']]\n",
    "display(set(df_ctdivol['Series Description']))\n",
    "display(set(df_ctdivol['Protocol Name']))\n",
    "display(set(df_ctdivol['Study Description']))\n",
    "display(df_ctdivol[(~df_ctdivol['Series Description'].str.match('.*low.*')) & (~df_ctdivol['Series Description'].str.match('.*Topogram.*'))])\n",
    "\n",
    "df_studies = pd.read_pickle('C:/TEMP/SchedVisu_data/studies.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Fetch more fields from the DICOMs #38\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "day = dt(2019, 2, 27)\n",
    "day_str = day.strftime('%Y%m%d')\n",
    "patientID = ['2026682', '138821']\n",
    "\n",
    "config = load_config()\n",
    "config['main']['start_date'] = day.strftime('%Y%m%d')\n",
    "config['main']['end_date'] = day.strftime('%Y%m%d')\n",
    "\n",
    "df_studies = find_studies_for_day(config, day_str, ['PT'])\n",
    "df_studies = df_studies.query('`Patient ID` in @patientID')\n",
    "\n",
    "df_series = find_series_for_studies(config, df_studies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Manually fetch the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_save = df_series.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_series, df_series_failed = fetch_info_for_series_with_batches(config, df_series_save.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for retrieving all data for calculating correct End Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the fetching so that it keeps all info for calculating end times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_and_save_single_day_data_from_PACS(config, datetime.datetime.strptime('20190111', '%Y%m%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline for a single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_logger()\n",
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-10'\n",
    "config['main']['end_date'] = '2019-01-10'\n",
    "\n",
    "retrieve_and_save_data_from_PACS(config)\n",
    "load_transform_and_save_data_from_files(config)\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_logger()\n",
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-09-05'\n",
    "config['main']['end_date'] = '2019-09-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrieve_and_save_single_day_data_from_PACS(config, datetime.datetime(2019, 9, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = extract_transform_and_save_data_from_files(config)\n",
    "#display(df)\n",
    "with pd.option_context(\"display.max_rows\", 1000): display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed_with_info = fetch_info_for_series(config, df_failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, df_failed_with_info], sort=True)\n",
    "df2.drop_duplicates('Series Instance UID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rescued_series = df_failed_with_info.copy()\n",
    "df_failed_series = df_rescued_series[\n",
    "            (df_rescued_series['Start Time'].isnull())\n",
    "            | (df_rescued_series['End Time'].isnull())\n",
    "            | (df_rescued_series['Machine'] == '')\n",
    "            | (df_rescued_series['Institution Name'] == '')]\n",
    "# exclude series where some information could still not be gathered (e.g. no end time or no machine)\n",
    "df_rescued_series = df_rescued_series.loc[~df_rescued_series.index.isin(df_failed_series.index), :]\n",
    "df2 = pd.concat([df, df_rescued_series], sort=True)\n",
    "df_failed_series\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-10'\n",
    "config['main']['end_date'] = '2019-01-10'\n",
    "df_studies = find_studies_for_day(config, '20190110', ['PT', 'NM'])\n",
    "df_studies = df_studies[df_studies['Patient ID'] == '2370187']\n",
    "df_all_series = find_series_for_studies(config, df_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_all_series[df_all_series['Number of Series Related Instances'].astype(int) > 1].Modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = df_all_series.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_series_subset = pd.concat([df_all_series[df_all_series.Modality == modality].head(n = 30) for modality in set(df_all_series.Modality)]).sort_values('Series Time').reset_index(drop=True)\n",
    "df_series = df_series_subset.copy()\n",
    "df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = fetch_info_for_series(config, df_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step before turing it to an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list of field names to extract for each modality\n",
    "to_fetch_fields_ctpt = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName',\n",
    "    'AcquisitionTime', 'Modality', 'ActualFrameDuration']\n",
    "to_fetch_fields_nm = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName',\n",
    "    'AcquisitionTime', 'Modality', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "# create modality specific masks of the DataFrame\n",
    "df_series_ctpt = df_series[df_series['Modality'].isin(['PT', 'CT'])]\n",
    "df_series_nm = df_series[df_series['Modality'] == 'NM']\n",
    "display(df_series_ctpt)\n",
    "display(df_series_nm)\n",
    "\n",
    "# prepare the CT/PT queries for the first instance (first image)\n",
    "query_dicts_ctpt = list(df_series_ctpt.apply(lambda row: {\n",
    "    'SeriesDate': row['Series Date'],\n",
    "    'PatientID': row['Patient ID'],\n",
    "    'SeriesInstanceUID': row['Series Instance UID'],\n",
    "    'InstanceNumber': '1'\n",
    "}, axis=1))\n",
    "# prepare the CT/PT queries for the last instance (last image)\n",
    "df_last_frames = df_series_ctpt[df_series_ctpt['Number of Series Related Instances'] != '1']\n",
    "if len(df_last_frames) > 0:\n",
    "    query_dicts_ctpt.extend(\n",
    "        df_last_frames.apply(lambda row: {\n",
    "            'SeriesDate': row['Series Date'],\n",
    "            'PatientID': row['Patient ID'],\n",
    "            'SeriesInstanceUID': row['Series Instance UID'],\n",
    "            'InstanceNumber': row['Number of Series Related Instances']\n",
    "        }, axis=1))\n",
    "# fetch the CT/PT data\n",
    "logging.info('Getting CT/PT data ({} queries)'.format(len(query_dicts_ctpt)))\n",
    "df_info_ctpt = get_data(config, query_dicts_ctpt, to_fetch_fields_ctpt)\n",
    "\n",
    "# prepare the NM queries for the first instance (first image)\n",
    "query_dicts_nm = list(df_series_nm.apply(lambda row: {\n",
    "    'SeriesDate': row['Series Date'],\n",
    "    'PatientID': row['Patient ID'],\n",
    "    'SeriesInstanceUID': row['Series Instance UID']\n",
    "}, axis=1))\n",
    "# fetch the NM data\n",
    "logging.info('Getting NM data ({} queries)'.format(len(query_dicts_nm)))\n",
    "df_info_nm = get_data(config, query_dicts_nm, to_fetch_fields_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 1000): display(df_info_ctpt.sort_values(['ManufacturerModelName', 'PatientID','SeriesInstanceUID', 'AcquisitionTime', 'InstanceNumber']))\n",
    "with pd.option_context(\"display.max_rows\", 1000): display(df_info_nm.sort_values(['ManufacturerModelName', 'PatientID', 'SeriesInstanceUID', 'Modality', 'AcquisitionTime', 'InstanceNumber']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the retrieved info DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_series_save = df_series.copy()\n",
    "df_info_ctpt_save = df_info_ctpt.copy()\n",
    "df_info_nm_save = df_info_nm.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually process the info and merge it back to the series DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series = df_series_save.copy()\n",
    "df_info_ctpt_save = df_info_ctpt_save.copy()\n",
    "df_info_nm_save = df_info_nm_save.copy()\n",
    "\n",
    "# Process PT/CT images\n",
    "if len(df_info_ctpt) > 0:\n",
    "\n",
    "    # get the images with a single instance\n",
    "    single_instances_UIDs = df_series.loc[\n",
    "        (df_series['Series Instance UID'].isin(df_info_ctpt['SeriesInstanceUID']))\\\n",
    "        & (df_series['Number of Series Related Instances'] == '1'), 'Series Instance UID']\n",
    "    logging.info('single_instances_UIDs')\n",
    "    display(single_instances_UIDs)\n",
    "    # duplicated them into the info DataFrame, so that they can also be merged together, as if there was two frames\n",
    "    df_info_ctpt_single_inst = df_info_ctpt[df_info_ctpt['SeriesInstanceUID'].isin(single_instances_UIDs)].copy()\n",
    "    df_info_ctpt_single_inst['InstanceNumber'] = 999999\n",
    "    df_info_ctpt_extended = pd.concat([df_info_ctpt, df_info_ctpt_single_inst], sort=True)\n",
    "    logging.info('df_info_ctpt_extended')\n",
    "    display(df_info_ctpt_extended)\n",
    "\n",
    "    # clean up the start times\n",
    "    df_info_ctpt_extended.loc[:, 'AcquisitionTime'] = df_info_ctpt_extended.loc[:, 'AcquisitionTime']\\\n",
    "        .apply(lambda t: str(t).split('.')[0])\n",
    "\n",
    "    # regroup the first and last instance rows on a single row\n",
    "    df_info_ctpt_merged = df_info_ctpt_extended[df_info_ctpt_extended['InstanceNumber'] == 1]\\\n",
    "        .merge(df_info_ctpt_extended[df_info_ctpt_extended['InstanceNumber'] > 1],\n",
    "               on=['SeriesInstanceUID', 'PatientID', 'ManufacturerModelName', 'Modality'],\n",
    "               suffixes=['_start', '_end'])\n",
    "    logging.info('df_info_ctpt_merged')\n",
    "    display(df_info_ctpt_merged)\n",
    "\n",
    "    # rename the columns and keep the appropriate ones\n",
    "    df_info_ctpt_clean = df_info_ctpt_merged.rename(columns={\n",
    "            'SeriesInstanceUID': 'Series Instance UID',\n",
    "            'PatientID': 'Patient ID',\n",
    "            'ManufacturerModelName': 'Machine',\n",
    "            'AcquisitionTime_start': 'Start Time',\n",
    "            'AcquisitionTime_end': 'End Time'})\\\n",
    "        .drop(columns=['InstanceNumber_start', 'InstanceNumber_end'])\n",
    "    logging.info('df_info_ctpt_clean')\n",
    "    display(df_info_ctpt_clean)\n",
    "    \n",
    "    s = pd.to_datetime(df_info_ctpt_clean['Start Time'], format='%H%M%S')\n",
    "    e = pd.to_datetime(df_info_ctpt_clean['End Time'], format='%H%M%S')\n",
    "    df_inv = df_info_ctpt_clean[s > e].copy()\n",
    "    df_inv[['Start Time','End Time']] = df_inv[['End Time','Start Time']]\n",
    "    df_info_ctpt_clean[s > e] = df_inv\n",
    "    logging.info('df_info_ctpt_clean 2')\n",
    "    display(df_info_ctpt_clean)\n",
    "    \n",
    "    # merge the info into the series DataFrame\n",
    "    df_series = df_series.merge(df_info_ctpt_clean, on=['Patient ID', 'Series Instance UID', 'Modality'], how='outer')\n",
    "    logging.info('df_series 1')\n",
    "    display(df_series)\n",
    "\n",
    "    # keep only the relevant columns\n",
    "    for f in ['Start Time', 'End Time', 'Machine']:\n",
    "        df_series[f] = df_series[f + '_y'].where(df_series[f + '_y'].notnull(), df_series[f + '_x'])\n",
    "        df_series.drop(columns=[f + '_y', f + '_x'], inplace=True)\n",
    "    logging.info('df_series post-CT')\n",
    "    display(df_series)\n",
    "\n",
    "# Process NM images\n",
    "if len(df_info_nm) > 0:\n",
    "    # clean up the start times\n",
    "    df_info_nm.loc[:, 'AcquisitionTime'] = df_info_nm.loc[:, 'AcquisitionTime']\\\n",
    "        .apply(lambda t: str(t).split('.')[0])\n",
    "    # use the AcquisitionTime as Start Time\n",
    "    df_info_nm['Start Time'] = df_info_nm['AcquisitionTime']\n",
    "    # call a function to calculate the End Times\n",
    "    df_info_nm['End Time'] = df_info_nm.apply(get_NM_series_end_time, axis=1)\n",
    "    # rename the columns and select the appropriate ones\n",
    "    df_info_nm_clean = df_info_nm.rename(columns={\n",
    "            'SeriesInstanceUID': 'Series Instance UID',\n",
    "            'PatientID': 'Patient ID',\n",
    "            'ManufacturerModelName': 'Machine'})\n",
    "    # merge the info into the series DataFrame\n",
    "    df_series = df_series.merge(df_info_nm_clean, on=['Patient ID', 'Series Instance UID', 'Modality'],\n",
    "        how='outer')\n",
    "    # keep only the relevant columns\n",
    "    for f in ['Start Time', 'End Time', 'Machine']:\n",
    "        df_series[f] = df_series[f + '_y'].where(df_series[f + '_y'].notnull(), df_series[f + '_x'])\n",
    "        df_series.drop(columns=[f + '_y', f + '_x'], inplace=True)\n",
    "    logging.info('df_series post-NM')\n",
    "    display(df_series)\n",
    "\n",
    "# remove duplicates\n",
    "df_series = df_series.drop_duplicates('Series Instance UID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series[['Patient ID', 'Modality', 'Start Time', 'End Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display(df_series)\n",
    "df_series.to_pickle('data/2019/2019-01/2019-01-10.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish the exploration after all this manual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "#df_series = load_data_from_files(config)\n",
    "# mark the rektakes and the machine group for each series\n",
    "df_series = mark_retakes(config, df_series)\n",
    "df_series = mark_machine_group(config, df_series)\n",
    "display(df_series[df_series['Patient ID'] == '2370187'])\n",
    "\n",
    "df_studies = df_series.replace(np.nan, '').groupby('SUID').agg({\n",
    "        'Series Date': lambda x: '/'.join(set(x)),\n",
    "        'Start Time': 'min',\n",
    "        'End Time': 'max',\n",
    "        'Study Description': lambda x: '/'.join(set(x)),\n",
    "        'Patient ID': lambda x: '/'.join(set(x)),\n",
    "        'Machine Group': lambda x: '/'.join(set(x)),\n",
    "        'Modality': lambda x: '/'.join(set(x)),\n",
    "        'Protocol Name': lambda x: '/'.join(set(x))\n",
    "    }).sort_values(['Series Date', 'Start Time', 'Machine Group', 'SUID'])\\\n",
    "    .rename(columns={'Series Date': 'Date'})\n",
    "\n",
    "df = df_studies[df_studies['Patient ID'] == '2370187'].copy()\n",
    "display(df)\n",
    "\n",
    "df['Machine'] = df['Machine Group'].str.replace('NoCT', '')\n",
    "df = df[df['Machine'] != 'mixed cases'].drop(columns='Machine Group')\n",
    "df.sort_values('Machine')[['Patient ID', 'Machine', 'Modality', 'Start Time', 'End Time', 'Study Description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test to query all images in one query data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all \"first\" CT/PT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_ctpt_first = []\n",
    "while len(df_info_ctpt_first) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_ctpt['Series Instance UID']))\n",
    "    ds.PatientID =  list(set(df_ctpt['Patient ID']))\n",
    "    ds.InstanceNumber = '1'\n",
    "    ds.Modality = ['CT','PT']\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_ctpt_first = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_info_ctpt_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all \"last\" CT/PT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_last = []\n",
    "while len(df_info_last) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_ctpt['Series Instance UID']))\n",
    "    ds.PatientID =  list(set(df_ctpt['Patient ID']))\n",
    "    ds.InstanceNumber = list(set(df_ctpt['Number of Series Related Instances']))\n",
    "    ds.Modality = ['CT','PT']\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_last = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_last.sort_values(by=\"InstanceNumber\").drop_duplicates(subset=[\"SeriesInstanceUID\"], keep=\"last\").reset_index()\n",
    "df_grouped.index += 1\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all NM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_nm = []\n",
    "while len(df_info_nm) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_nm['Series Instance UID'][0:5]))\n",
    "    ds.PatientID =  list(set(df_nm['Patient ID'][0:5]))\n",
    "    ds.Modality = 'NM'\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_nm = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Merge the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_series.merge(df_info.drop(columns='Modality').rename(columns={'SeriesInstanceUID': 'Series Instance UID'}), on='Series Instance UID', how='outer')\n",
    "#df_merged[['Series Date', 'Series Time', 'AcquisitionTime']]\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = []\n",
    "while len(df_info) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesInstanceUID = df_series['Series Instance UID']\n",
    "    ds.PatientID =  df_series['Patient ID']\n",
    "    ds.Modality = 'NM'\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info = get_data(config, [ds], to_fetch_fields)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
