{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import os\n",
    "os.chdir('C:/TEMP/pySchedVisu/code')\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from main import run, load_config, get_day_range\n",
    "from retrieve_data import *\n",
    "from extract_data import load_transform_and_save_data_from_files, add_preparation_times\n",
    "from create_report import create_report, get_report_type\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set the width of the notebook\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Number of slots per day #48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191125'\n",
    "config['main']['end_date'] = '20191213'\n",
    "machine = 'Intevo,Millennium'\n",
    "config['draw']['debug_single_machine'] = machine\n",
    "config['main']['mode'] = 'single'\n",
    "config['draw']['debug_save_as_image'] = 'True'\n",
    "config['draw']['dpi'] = '100'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191209'\n",
    "config['main']['end_date'] = '20191213'\n",
    "machine = 'Intevo'\n",
    "config['draw']['debug_single_machine'] = machine\n",
    "config['main']['mode'] = 'single'\n",
    "config['draw']['debug_save_as_image'] = 'True'\n",
    "config['draw']['dpi'] = '100'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date_weekday'] = list(pd.to_datetime(df['Date']).apply(lambda x: x.weekday()))\n",
    "n_slots_per_day = pd.Series(config['draw']['n_study_per_day_' + machine.lower().replace(' ', '')].split(',')).astype(int)\n",
    "df['Date_weekday'].apply(lambda x: n_slots_per_day[x]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 25): display(df.groupby(['Machine', 'Date_weekday'])['Date'].count())\n",
    "with pd.option_context('display.max_rows', 25): display(df.groupby(['Machine', 'Date'])['Date'].count().groupby(['Machine']).count())\n",
    "with pd.option_context('display.max_rows', 25): display(df.groupby(['Machine', 'Date_weekday'])['Date'].count() / df.groupby(['Machine', 'Date'])['Date'].count().groupby(['Machine']).count() * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Split OS3PHASES into two parts #27\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20190101'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['draw']['debug_single_machine'] = 'Discovery 670 NM'\n",
    "config['main']['mode'] = 'single'\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)\n",
    "df_studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_times = pd.to_datetime(df_studies['Start Time'], format='%H%M%S')\n",
    "end_times = pd.to_datetime(df_studies['End Time'], format='%H%M%S')\n",
    "df_studies['duration'] = end_times - start_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_studies['i_take'] = list(df_studies.reset_index()['SUID'].apply(lambda x: x.split('_')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies.groupby(['Description', 'i_take']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies.groupby(['Description', 'i_take'])['duration'].agg(duration=pd.Series.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191209'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['draw']['debug_single_machine'] = 'Discovery 670 NM'\n",
    "config['main']['mode'] = 'single'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Add grid lines for each hour behind the schedule plot #35\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20190902'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['main']['mode'] = 'single'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for weird overlappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191202'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['main']['mode'] = 'single'\n",
    "config['draw']['debug_single_machine'] = 'Discovery 670 NM'\n",
    "config['draw']['debug_schedule_show_IPP_string'] = 'False'\n",
    "config['draw']['debug_save_as_image'] = 'False'\n",
    "config['draw']['dpi'] = '150'\n",
    "config['extract']['debug_force_extract_studies'] = 'False'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from extract_data import mark_retakes\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)\n",
    "df = df_series.query('`Machine Group` == \"Discovery 670 NM\" and Date == \"20191212\"')\n",
    "with pd.option_context('display.max_colwidth', -1): display(df[['Patient ID', 'Modality', 'Start Time', 'End Time', 'Series Description', 'ImageType', 'Study Instance UID', 'i_take']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_studies.query('Machine == \"Discovery 670 NM\" and Date == \"20191212\" and `Patient ID` == \"2948602\"')\n",
    "display(df[['Modality', 'Start Time', 'End Time', 'Description']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Calculate inter study time #49\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191209'\n",
    "config['main']['end_date'] = '20191213'\n",
    "config['main']['mode'] = 'single'\n",
    "df_all, _ = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query('Machine == \"Discovery 670 NM\" and Date >= \"20191209\" and Date <= \"20191213\"').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FMT = '%H%M%S'\n",
    "df['Start Time'] = pd.to_datetime(df['Start Time'], format=FMT)\n",
    "df['End Time'] = pd.to_datetime(df['End Time'], format=FMT)\n",
    "df['Start Time Prep'] = pd.to_datetime(df['Start Time Prep'], format=FMT)\n",
    "df['End Time Prep'] = pd.to_datetime(df['End Time Prep'], format=FMT)\n",
    "# compare the start time of a row with the end time of the previous row\n",
    "df['time_to_prev'] = df['End Time'].shift() - df['Start Time']\n",
    "df.loc[df['time_to_prev'] < timedelta(0), 'time_to_prev'] *= -1\n",
    "df['time_to_prev_prep'] = df['End Time Prep'].shift() - df['Start Time Prep']\n",
    "df.loc[df['time_to_prev_prep'] < timedelta(0), 'time_to_prev_prep'] *= -1\n",
    "# compare the end time of a row with the start time of the next row\n",
    "df['time_to_next'] = df['Start Time'].shift(-1) - df['End Time']\n",
    "df.loc[df['time_to_next'] < timedelta(0), 'time_to_next'] *= -1\n",
    "df['time_to_next_prep'] = df['Start Time Prep'].shift(-1) - df['End Time Prep']\n",
    "df.loc[df['time_to_next_prep'] < timedelta(0), 'time_to_next_prep'] *= -1\n",
    "# get the fully contained rows\n",
    "df['fully_contained'] = (df['End Time'] < df['End Time'].shift()) & (df['Start Time'] > df['Start Time'].shift()) & (df['Date'].shift() == df['Date']) & (df['Date'].shift(-1) == df['Date'])\n",
    "# make sure that we only keep values where the dates are identical\n",
    "df.loc[df['Date'] != df['Date'].shift(), 'time_to_prev'] = pd.NaT\n",
    "df.loc[df['Date'] != df['Date'].shift(), 'time_to_prev_prep'] = pd.NaT\n",
    "df.loc[df['Date'] != df['Date'].shift(-1), 'time_to_next'] = pd.NaT\n",
    "df.loc[df['Date'] != df['Date'].shift(-1), 'time_to_next_prep'] = pd.NaT\n",
    "# get the average times\n",
    "average_times = df.groupby(['Machine', 'Description'])[['time_to_prev', 'time_to_prev_prep', 'time_to_next', 'time_to_next_prep']].agg(pd.Series.mean)\n",
    "average_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Add preparation times around study blocks #21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191125'\n",
    "config['main']['end_date'] = '20191125'\n",
    "config['main']['mode'] = 'single'\n",
    "df_all, _ = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query('Machine == \"PET GE\" and Date == \"20191125\"').copy()\n",
    "display(df)\n",
    "df = add_preparation_times(config, df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191125'\n",
    "config['main']['end_date'] = '20191125'\n",
    "config['main']['mode'] = 'single'\n",
    "config['draw']['debug_single_machine'] = 'PET GE'\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Add bookmarks to PDF #53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "config = load_config()\n",
    "pdf_output_path, bookmarks = create_report(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the handles of the created bookmarks\n",
    "bookmark_handles = {}\n",
    "\n",
    "# read in the input file and copy it to output while adding bookmarks\n",
    "with open(pdf_output_path, 'rb') as input_file:\n",
    "    # define the input and output objects\n",
    "    reader = PdfFileReader(input_file)\n",
    "    writer = PdfFileWriter()\n",
    "    # copy meta data\n",
    "    metadata = reader.getDocumentInfo()\n",
    "    writer.addMetadata(metadata)\n",
    "    # go through the bookmarks\n",
    "    i_page, parent_bookmark_handle = 0, None\n",
    "    for bookmark in bookmarks:\n",
    "        # if we encounter a page we did not copy yet, add it\n",
    "        if i_page == bookmark.page:\n",
    "            writer.addPage(reader.getPage(i_page))\n",
    "            i_page += 1\n",
    "            print(i_page)\n",
    "        # if the bookmark has the previous bookmark as a parent\n",
    "        if bookmark.parent is not None and bookmark.parent in bookmark_handles.keys():\n",
    "            bookmark_handles[bookmark.title] = \\\n",
    "                writer.addBookmark(bookmark.title, bookmark.page, bookmark_handles[bookmark.parent])\n",
    "        else:\n",
    "            bookmark_handles[bookmark.title] = writer.addBookmark(bookmark.title, bookmark.page)\n",
    "\n",
    "# write out the file\n",
    "with open(pdf_output_path.replace('.pdf', '2.pdf'), 'wb') as out:\n",
    "    writer.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Make report send email #54\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "config = load_config()\n",
    "now_str = dt.now().strftime('%Y-%m-%d_%Hh%Mm%Ss')\n",
    "pdf_output_path = '{}/report_{}.pdf'.format(config['path']['output_dir'], now_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relevant part of the config\n",
    "email = config['email']\n",
    "\n",
    "# get the body and the subject of the mail\n",
    "body = email['body']\n",
    "subject = email['subject']\n",
    "\n",
    "# split the path into pieces\n",
    "report_file_name = pdf_output_path.split('/')[-1]\n",
    "report_folder_path = pdf_output_path.split('/')[:-1]\n",
    "\n",
    "# replace the relevant parts \n",
    "body = body.replace('{__REPORT_PATH__}', pdf_output_path)\n",
    "body = body.replace('{__REPORT_FOLDER_PATH__}', pdf_output_path)\n",
    "body = body.replace('{__REPORT_FILE_NAME__}', pdf_output_path)\n",
    "\n",
    "\n",
    "print(subject)\n",
    "print(body)\n",
    "return\n",
    "# create the body and populate it with headers\n",
    "msg = MIMEText(body, 'html')\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = email['sender_name'] + '<' + email['sender_email'] + '>'\n",
    "msg['To'] = email['recipients_email']\n",
    "\n",
    "# create the connection to the server and send the mail\n",
    "s = smtplib.SMTP(email['smtp_server'])\n",
    "s.sendmail(email['sender_email'], email['recipients_email'].split(','), msg.as_string())\n",
    "# close the sessions\n",
    "s.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "output = PdfFileWriter()\n",
    "input1 = PdfFileReader(open('C:/TEMP/SchedVisu/outputs/report_2019-12-18_10h47m03s.pdf', 'rb'))\n",
    "output.addPage(input1.getPage(0))\n",
    "output.addPage(input1.getPage(1))\n",
    "output.addPage(input1.getPage(2))\n",
    "parent = output.addBookmark('TestParent', 0) # add parent bookmark\n",
    "output.addBookmark('TestChild1', 0, parent) # add child bookmark\n",
    "output.addBookmark('TestChild2', 1, parent) # add child bookmark\n",
    "output.addBookmark('TestChild3', 2, parent) # add child bookmark\n",
    "with open('C:/TEMP/SchedVisu/outputs/test.pdf', 'wb') as out:\n",
    "    output.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tests for comparing to VENUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from create_report import _set_schedule_y_lims\n",
    "from extract_data import load_transform_and_save_data_from_files\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.colors as mc\n",
    "import colorsys\n",
    "\n",
    "config = load_config()\n",
    "config['main']['start_date'] = '20191205'\n",
    "config['main']['end_date'] = '20191205'\n",
    "\n",
    "# either go through all available machines, or use the list specified by the config\n",
    "machines_list = sorted(list(set([machine for machine in config['machines'].keys() if 'NoCT' not in machine])))\n",
    "if config['draw']['debug_single_machine'] != '*':\n",
    "    machines_list = config['draw']['debug_single_machine'].split(',')\n",
    "\n",
    "# get the starting and ending dates, and the days range from the config\n",
    "start_date, end_date, days_range = main.get_day_range(config)\n",
    "\n",
    "# create a matplotlib figure with the right aspect ratio\n",
    "fig = plt.figure(figsize=[8.27, 11.69])\n",
    "    \n",
    "# create the new axes\n",
    "sched_ax = fig.add_axes([0.06, 0.42, 0.80, 0.39], anchor='NE')\n",
    "sched_ax.invert_yaxis()\n",
    "\n",
    "# create the ticks and labels, with a reduced frequency\n",
    "_, _, days_range_xticks = main.get_day_range(config, reduce_freq=True)\n",
    "days_xticks, days_xtick_labels = [], []\n",
    "\n",
    "# plot each day\n",
    "i_day = 0\n",
    "n_days_to_show = len(days_range)\n",
    "\n",
    "df, _ = load_transform_and_save_data_from_files(config)\n",
    "# exclude some machines and do some grouping up\n",
    "df['Machine'] = df['Machine Group'].str.replace('NoCT', '')\n",
    "df = df[df['Machine'] != 'mixed cases']\n",
    "    \n",
    "# go through each machine\n",
    "for machine in machines_list:\n",
    "\n",
    "    # get the data for the current day and machine\n",
    "    df_day = df.query('Machine == \"{}\"'.format(machine))\n",
    "\n",
    "    # go through each study found for this machine\n",
    "    for i_study in range(len(df_day)):\n",
    "        study = df_day.iloc[i_study, :]\n",
    "\n",
    "        # get the start time, end time and duration as hours with decimals\n",
    "        start = pd.to_datetime(study['Start Time'], format='%H%M%S')\n",
    "        end = pd.to_datetime(study['End Time'], format='%H%M%S')\n",
    "        start_hour = start.hour + start.minute / 60 + start.second / 3600\n",
    "        end_hour = end.hour + end.minute / 60 + end.second / 3600\n",
    "        duration_hours = end_hour - start_hour\n",
    "\n",
    "        # if the duration is negative\n",
    "        if duration_hours <= 0: continue\n",
    "\n",
    "        # get the coordinates where the rounded rectangle for this study should be plotted\n",
    "        box_w = config['draw'].getfloat('study_box_w')\n",
    "        x_shift = config['draw'].getfloat('study_x_shift')\n",
    "        x = i_day - (box_w * 0.5) + (x_shift * (-1 if (i_study % 2 == 0) else 1))\n",
    "        y, w, h = start_hour, box_w, duration_hours\n",
    "\n",
    "        # check if we have an overlap issue\n",
    "        if i_study > 0:\n",
    "            end_prev = pd.to_datetime(df_day.iloc[i_study - 1, :]['End Time'], format='%H%M%S')\n",
    "            end_prev_hour = end_prev.hour + end_prev.minute / 60 + end_prev.second / 3600\n",
    "            \n",
    "            # check how long the gap was with previous study\n",
    "            gap_duration_hour = start_hour - end_prev_hour\n",
    "            gap_threshold = config['draw'].getfloat('gap_dur_minutes_' + machine.lower().replace(' ', ''))\n",
    "            if gap_duration_hour * 60 >= gap_threshold:\n",
    "                # plot a black line to show gaps\n",
    "                plt.plot([i_day, i_day], [start_hour - 0.15, end_prev_hour + 0.15],\n",
    "                    color='black', linestyle='dashed', linewidth=2)\n",
    "\n",
    "        # define colors\n",
    "        descr_list = list(config['description_' + machine.lower().replace(' ', '')].keys()) + ['OTHER']\n",
    "        colors = colors = config['draw']['colors'].split(',')\n",
    "        i_descr = descr_list.index(study['Description'])\n",
    "\n",
    "        # check if the current study is a retake\n",
    "        try:\n",
    "            i_take = int(study.name.split('_')[-1])\n",
    "        except ValueError:\n",
    "            logging.warning('Problem with study ...{} on {}: got a weird retake number: \"{}\"'\n",
    "                .format('.'.join(study.name.split('.')[-2:]), machine,  study.name))\n",
    "            i_take = 1\n",
    "\n",
    "        hatch, edge_color = '', 'black'\n",
    "        if i_take != 1:\n",
    "            logging.debug(study.name + ' is a retake (reprise)')\n",
    "            hatch = '/'\n",
    "            edge_color = 'red'\n",
    "            sibling_studies_patches = [\n",
    "                    p for p in sched_ax.patches\n",
    "                    if p._label.split('_')[0] == study.name.split('_')[0]\n",
    "                ]\n",
    "            for p in sibling_studies_patches:\n",
    "                p.set_hatch('\\\\')\n",
    "                p.set_edgecolor('red')\n",
    "\n",
    "        # if we are displaying more than ~4 months, the inside of the blocks is not visible anymore.\n",
    "        #   Therefore, we need to use the edge to show the colors\n",
    "        if len(days_range) > 95:\n",
    "            edge_color = colors[i_descr]\n",
    "            hatch = ''\n",
    "\n",
    "        # create the shape and plot it\n",
    "        rounded_rect = FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=-0.0040,rounding_size=0.155\",\n",
    "            fc=colors[i_descr], ec=edge_color, mutation_aspect=0.3, hatch=hatch, label=study.name)\n",
    "        sched_ax.add_patch(rounded_rect)\n",
    "\n",
    "        # DEBUG show information string\n",
    "        if config['draw'].getboolean('debug_schedule_show_IPP_string'):\n",
    "            plt.text(x + w * 0.1, y + 0.9 * h, study['Patient ID'], fontsize=8)\n",
    "\n",
    "    i_day += 1\n",
    "    \n",
    "plt.xlim([-0.5, i_day - 0.5])\n",
    "plt.xticks(range(i_day), machines_list)\n",
    "# set the y limits\n",
    "_set_schedule_y_lims(config, df)\n",
    "\n",
    "fig.savefig('schedvisu_20191205.png', orientation='portrait', papertype='a4', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for CTDIvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_all = pd.read_pickle('C:/TEMP/SchedVisu/data/series.pkl')\n",
    "df_all2019 = df_all.query('Date > \"20191101\"').copy()\n",
    "df_noctdi = df_all2019[df_all2019['CTDIvol'].isnull()].copy()\n",
    "df = df_all2019[~df_all2019['CTDIvol'].isnull()].copy()\n",
    "print(f'Found {len(df)} series with CTDIvol information out of {len(df_all2019)} series ({len(df_noctdi)} have no information)')\n",
    "df = df.rename(columns={'CTDIvol':'CTDIvol_ori'})\n",
    "df['CTDIvol'] = df['CTDIvol_ori'].apply(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_data import create_description_consensus\n",
    "df['Study Description'] = df['Study Description'].replace(np.nan, '')\n",
    "df = create_description_consensus(config, df)\n",
    "df_all2019['Study Description'] = df_all2019['Study Description'].replace(np.nan, '')\n",
    "df_all2019 = create_description_consensus(config, df_all2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df[['Patient ID', 'Date', 'Modality', 'Start Time', 'End Time', 'Description', 'Study Description', 'Series Description', 'Machine', 'CTDIvol', 'CTDIvol_ori', 'ImageType']].to_excel('CTDI_vol.xls')\n",
    "df_all2019[['Patient ID', 'Date', 'Modality', 'Start Time', 'End Time', 'Description', 'Study Description', 'Series Description', 'Machine', 'CTDIvol', 'ImageType']].to_excel('CTDI_vol_all.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[['CTDIvol', 'Description', 'Patient ID']].groupby(['CTDIvol', 'Description'])['Patient ID'].count().sort_values(ascending=False).to_csv('ctdivol_count_description.csv', header=True)\n",
    "df[['CTDIvol', 'Series Description', 'Patient ID']].groupby(['CTDIvol', 'Series Description'])['Patient ID'].count().sort_values(ascending=False).to_csv('ctdivol_count_series_description.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('max_rows', -1): display(df.groupby(['CTDIvol', 'Series Description'])['Patient ID'].count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('max_rows', -1): display(df.groupby(['CTDIvol', 'Description'])['Patient ID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for other exam types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/TEMP/SchedVisu/data/studies.pkl')\n",
    "descr = df.query('Description == \"OTHER\" and Machine == \"Millennium\"').groupby('Study Description')['Date'].count().sort_values()\n",
    "descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for creating a multi-page PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Report Type classification wrong #36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20191125'\n",
    "config['main']['end_date'] = '20191219'\n",
    "start_date, end_date, _ = main.get_day_range(config)\n",
    "print((end_date - start_date).days)\n",
    "print(end_date - start_date)\n",
    "get_report_type(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Dose report #5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series = pd.read_pickle('C:/TEMP/SchedVisu_data/series.pkl')\n",
    "print(df_series.columns)\n",
    "df_ctdivol = df_series[(~df_series['CTDIvol'].isnull()) | (~df_series['CTDIvol_start'].isnull())][['Date', 'CTDIvol_start', 'Series Description', 'Study Description', 'Protocol Name', 'Modality', 'Patient ID', 'ImageType']]\n",
    "display(set(df_ctdivol['Series Description']))\n",
    "display(set(df_ctdivol['Protocol Name']))\n",
    "display(set(df_ctdivol['Study Description']))\n",
    "display(df_ctdivol[(~df_ctdivol['Series Description'].str.match('.*low.*')) & (~df_ctdivol['Series Description'].str.match('.*Topogram.*'))])\n",
    "\n",
    "df_studies = pd.read_pickle('C:/TEMP/SchedVisu_data/studies.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for \"Fetch more fields from the DICOMs #38\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "day = dt(2019, 2, 27)\n",
    "day_str = day.strftime('%Y%m%d')\n",
    "patientID = ['2026682', '138821']\n",
    "\n",
    "config = load_config()\n",
    "config['main']['start_date'] = day.strftime('%Y%m%d')\n",
    "config['main']['end_date'] = day.strftime('%Y%m%d')\n",
    "\n",
    "df_studies = find_studies_for_day(config, day_str, ['PT'])\n",
    "df_studies = df_studies.query('`Patient ID` in @patientID')\n",
    "\n",
    "df_series = find_series_for_studies(config, df_studies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Manually fetch the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_save = df_series.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_series, df_series_failed = fetch_info_for_series_with_batches(config, df_series_save.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for retrieving all data for calculating correct End Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the fetching so that it keeps all info for calculating end times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_and_save_single_day_data_from_PACS(config, datetime.datetime.strptime('20190111', '%Y%m%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline for a single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_logger()\n",
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-10'\n",
    "config['main']['end_date'] = '2019-01-10'\n",
    "\n",
    "retrieve_and_save_data_from_PACS(config)\n",
    "load_transform_and_save_data_from_files(config)\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_logger()\n",
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-09-05'\n",
    "config['main']['end_date'] = '2019-09-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrieve_and_save_single_day_data_from_PACS(config, datetime.datetime(2019, 9, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = extract_transform_and_save_data_from_files(config)\n",
    "#display(df)\n",
    "with pd.option_context(\"display.max_rows\", 1000): display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed_with_info = fetch_info_for_series(config, df_failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, df_failed_with_info], sort=True)\n",
    "df2.drop_duplicates('Series Instance UID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rescued_series = df_failed_with_info.copy()\n",
    "df_failed_series = df_rescued_series[\n",
    "            (df_rescued_series['Start Time'].isnull())\n",
    "            | (df_rescued_series['End Time'].isnull())\n",
    "            | (df_rescued_series['Machine'] == '')\n",
    "            | (df_rescued_series['Institution Name'] == '')]\n",
    "# exclude series where some information could still not be gathered (e.g. no end time or no machine)\n",
    "df_rescued_series = df_rescued_series.loc[~df_rescued_series.index.isin(df_failed_series.index), :]\n",
    "df2 = pd.concat([df, df_rescued_series], sort=True)\n",
    "df_failed_series\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-10'\n",
    "config['main']['end_date'] = '2019-01-10'\n",
    "df_studies = find_studies_for_day(config, '20190110', ['PT', 'NM'])\n",
    "df_studies = df_studies[df_studies['Patient ID'] == '2370187']\n",
    "df_all_series = find_series_for_studies(config, df_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_all_series[df_all_series['Number of Series Related Instances'].astype(int) > 1].Modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = df_all_series.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_series_subset = pd.concat([df_all_series[df_all_series.Modality == modality].head(n = 30) for modality in set(df_all_series.Modality)]).sort_values('Series Time').reset_index(drop=True)\n",
    "df_series = df_series_subset.copy()\n",
    "df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = fetch_info_for_series(config, df_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step before turing it to an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list of field names to extract for each modality\n",
    "to_fetch_fields_ctpt = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName',\n",
    "    'AcquisitionTime', 'Modality', 'ActualFrameDuration']\n",
    "to_fetch_fields_nm = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName',\n",
    "    'AcquisitionTime', 'Modality', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "# create modality specific masks of the DataFrame\n",
    "df_series_ctpt = df_series[df_series['Modality'].isin(['PT', 'CT'])]\n",
    "df_series_nm = df_series[df_series['Modality'] == 'NM']\n",
    "display(df_series_ctpt)\n",
    "display(df_series_nm)\n",
    "\n",
    "# prepare the CT/PT queries for the first instance (first image)\n",
    "query_dicts_ctpt = list(df_series_ctpt.apply(lambda row: {\n",
    "    'SeriesDate': row['Series Date'],\n",
    "    'PatientID': row['Patient ID'],\n",
    "    'SeriesInstanceUID': row['Series Instance UID'],\n",
    "    'InstanceNumber': '1'\n",
    "}, axis=1))\n",
    "# prepare the CT/PT queries for the last instance (last image)\n",
    "df_last_frames = df_series_ctpt[df_series_ctpt['Number of Series Related Instances'] != '1']\n",
    "if len(df_last_frames) > 0:\n",
    "    query_dicts_ctpt.extend(\n",
    "        df_last_frames.apply(lambda row: {\n",
    "            'SeriesDate': row['Series Date'],\n",
    "            'PatientID': row['Patient ID'],\n",
    "            'SeriesInstanceUID': row['Series Instance UID'],\n",
    "            'InstanceNumber': row['Number of Series Related Instances']\n",
    "        }, axis=1))\n",
    "# fetch the CT/PT data\n",
    "logging.info('Getting CT/PT data ({} queries)'.format(len(query_dicts_ctpt)))\n",
    "df_info_ctpt = get_data(config, query_dicts_ctpt, to_fetch_fields_ctpt)\n",
    "\n",
    "# prepare the NM queries for the first instance (first image)\n",
    "query_dicts_nm = list(df_series_nm.apply(lambda row: {\n",
    "    'SeriesDate': row['Series Date'],\n",
    "    'PatientID': row['Patient ID'],\n",
    "    'SeriesInstanceUID': row['Series Instance UID']\n",
    "}, axis=1))\n",
    "# fetch the NM data\n",
    "logging.info('Getting NM data ({} queries)'.format(len(query_dicts_nm)))\n",
    "df_info_nm = get_data(config, query_dicts_nm, to_fetch_fields_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 1000): display(df_info_ctpt.sort_values(['ManufacturerModelName', 'PatientID','SeriesInstanceUID', 'AcquisitionTime', 'InstanceNumber']))\n",
    "with pd.option_context(\"display.max_rows\", 1000): display(df_info_nm.sort_values(['ManufacturerModelName', 'PatientID', 'SeriesInstanceUID', 'Modality', 'AcquisitionTime', 'InstanceNumber']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the retrieved info DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_series_save = df_series.copy()\n",
    "df_info_ctpt_save = df_info_ctpt.copy()\n",
    "df_info_nm_save = df_info_nm.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually process the info and merge it back to the series DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_series = df_series_save.copy()\n",
    "df_info_ctpt_save = df_info_ctpt_save.copy()\n",
    "df_info_nm_save = df_info_nm_save.copy()\n",
    "\n",
    "# Process PT/CT images\n",
    "if len(df_info_ctpt) > 0:\n",
    "\n",
    "    # get the images with a single instance\n",
    "    single_instances_UIDs = df_series.loc[\n",
    "        (df_series['Series Instance UID'].isin(df_info_ctpt['SeriesInstanceUID']))\\\n",
    "        & (df_series['Number of Series Related Instances'] == '1'), 'Series Instance UID']\n",
    "    logging.info('single_instances_UIDs')\n",
    "    display(single_instances_UIDs)\n",
    "    # duplicated them into the info DataFrame, so that they can also be merged together, as if there was two frames\n",
    "    df_info_ctpt_single_inst = df_info_ctpt[df_info_ctpt['SeriesInstanceUID'].isin(single_instances_UIDs)].copy()\n",
    "    df_info_ctpt_single_inst['InstanceNumber'] = 999999\n",
    "    df_info_ctpt_extended = pd.concat([df_info_ctpt, df_info_ctpt_single_inst], sort=True)\n",
    "    logging.info('df_info_ctpt_extended')\n",
    "    display(df_info_ctpt_extended)\n",
    "\n",
    "    # clean up the start times\n",
    "    df_info_ctpt_extended.loc[:, 'AcquisitionTime'] = df_info_ctpt_extended.loc[:, 'AcquisitionTime']\\\n",
    "        .apply(lambda t: str(t).split('.')[0])\n",
    "\n",
    "    # regroup the first and last instance rows on a single row\n",
    "    df_info_ctpt_merged = df_info_ctpt_extended[df_info_ctpt_extended['InstanceNumber'] == 1]\\\n",
    "        .merge(df_info_ctpt_extended[df_info_ctpt_extended['InstanceNumber'] > 1],\n",
    "               on=['SeriesInstanceUID', 'PatientID', 'ManufacturerModelName', 'Modality'],\n",
    "               suffixes=['_start', '_end'])\n",
    "    logging.info('df_info_ctpt_merged')\n",
    "    display(df_info_ctpt_merged)\n",
    "\n",
    "    # rename the columns and keep the appropriate ones\n",
    "    df_info_ctpt_clean = df_info_ctpt_merged.rename(columns={\n",
    "            'SeriesInstanceUID': 'Series Instance UID',\n",
    "            'PatientID': 'Patient ID',\n",
    "            'ManufacturerModelName': 'Machine',\n",
    "            'AcquisitionTime_start': 'Start Time',\n",
    "            'AcquisitionTime_end': 'End Time'})\\\n",
    "        .drop(columns=['InstanceNumber_start', 'InstanceNumber_end'])\n",
    "    logging.info('df_info_ctpt_clean')\n",
    "    display(df_info_ctpt_clean)\n",
    "    \n",
    "    s = pd.to_datetime(df_info_ctpt_clean['Start Time'], format='%H%M%S')\n",
    "    e = pd.to_datetime(df_info_ctpt_clean['End Time'], format='%H%M%S')\n",
    "    df_inv = df_info_ctpt_clean[s > e].copy()\n",
    "    df_inv[['Start Time','End Time']] = df_inv[['End Time','Start Time']]\n",
    "    df_info_ctpt_clean[s > e] = df_inv\n",
    "    logging.info('df_info_ctpt_clean 2')\n",
    "    display(df_info_ctpt_clean)\n",
    "    \n",
    "    # merge the info into the series DataFrame\n",
    "    df_series = df_series.merge(df_info_ctpt_clean, on=['Patient ID', 'Series Instance UID', 'Modality'], how='outer')\n",
    "    logging.info('df_series 1')\n",
    "    display(df_series)\n",
    "\n",
    "    # keep only the relevant columns\n",
    "    for f in ['Start Time', 'End Time', 'Machine']:\n",
    "        df_series[f] = df_series[f + '_y'].where(df_series[f + '_y'].notnull(), df_series[f + '_x'])\n",
    "        df_series.drop(columns=[f + '_y', f + '_x'], inplace=True)\n",
    "    logging.info('df_series post-CT')\n",
    "    display(df_series)\n",
    "\n",
    "# Process NM images\n",
    "if len(df_info_nm) > 0:\n",
    "    # clean up the start times\n",
    "    df_info_nm.loc[:, 'AcquisitionTime'] = df_info_nm.loc[:, 'AcquisitionTime']\\\n",
    "        .apply(lambda t: str(t).split('.')[0])\n",
    "    # use the AcquisitionTime as Start Time\n",
    "    df_info_nm['Start Time'] = df_info_nm['AcquisitionTime']\n",
    "    # call a function to calculate the End Times\n",
    "    df_info_nm['End Time'] = df_info_nm.apply(get_NM_series_end_time, axis=1)\n",
    "    # rename the columns and select the appropriate ones\n",
    "    df_info_nm_clean = df_info_nm.rename(columns={\n",
    "            'SeriesInstanceUID': 'Series Instance UID',\n",
    "            'PatientID': 'Patient ID',\n",
    "            'ManufacturerModelName': 'Machine'})\n",
    "    # merge the info into the series DataFrame\n",
    "    df_series = df_series.merge(df_info_nm_clean, on=['Patient ID', 'Series Instance UID', 'Modality'],\n",
    "        how='outer')\n",
    "    # keep only the relevant columns\n",
    "    for f in ['Start Time', 'End Time', 'Machine']:\n",
    "        df_series[f] = df_series[f + '_y'].where(df_series[f + '_y'].notnull(), df_series[f + '_x'])\n",
    "        df_series.drop(columns=[f + '_y', f + '_x'], inplace=True)\n",
    "    logging.info('df_series post-NM')\n",
    "    display(df_series)\n",
    "\n",
    "# remove duplicates\n",
    "df_series = df_series.drop_duplicates('Series Instance UID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series[['Patient ID', 'Modality', 'Start Time', 'End Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display(df_series)\n",
    "df_series.to_pickle('data/2019/2019-01/2019-01-10.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish the exploration after all this manual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "#df_series = load_data_from_files(config)\n",
    "# mark the rektakes and the machine group for each series\n",
    "df_series = mark_retakes(config, df_series)\n",
    "df_series = mark_machine_group(config, df_series)\n",
    "display(df_series[df_series['Patient ID'] == '2370187'])\n",
    "\n",
    "df_studies = df_series.replace(np.nan, '').groupby('SUID').agg({\n",
    "        'Series Date': lambda x: '/'.join(set(x)),\n",
    "        'Start Time': 'min',\n",
    "        'End Time': 'max',\n",
    "        'Study Description': lambda x: '/'.join(set(x)),\n",
    "        'Patient ID': lambda x: '/'.join(set(x)),\n",
    "        'Machine Group': lambda x: '/'.join(set(x)),\n",
    "        'Modality': lambda x: '/'.join(set(x)),\n",
    "        'Protocol Name': lambda x: '/'.join(set(x))\n",
    "    }).sort_values(['Series Date', 'Start Time', 'Machine Group', 'SUID'])\\\n",
    "    .rename(columns={'Series Date': 'Date'})\n",
    "\n",
    "df = df_studies[df_studies['Patient ID'] == '2370187'].copy()\n",
    "display(df)\n",
    "\n",
    "df['Machine'] = df['Machine Group'].str.replace('NoCT', '')\n",
    "df = df[df['Machine'] != 'mixed cases'].drop(columns='Machine Group')\n",
    "df.sort_values('Machine')[['Patient ID', 'Machine', 'Modality', 'Start Time', 'End Time', 'Study Description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test to query all images in one query data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all \"first\" CT/PT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_ctpt_first = []\n",
    "while len(df_info_ctpt_first) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_ctpt['Series Instance UID']))\n",
    "    ds.PatientID =  list(set(df_ctpt['Patient ID']))\n",
    "    ds.InstanceNumber = '1'\n",
    "    ds.Modality = ['CT','PT']\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_ctpt_first = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_info_ctpt_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all \"last\" CT/PT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_last = []\n",
    "while len(df_info_last) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_ctpt['Series Instance UID']))\n",
    "    ds.PatientID =  list(set(df_ctpt['Patient ID']))\n",
    "    ds.InstanceNumber = list(set(df_ctpt['Number of Series Related Instances']))\n",
    "    ds.Modality = ['CT','PT']\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_last = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_last.sort_values(by=\"InstanceNumber\").drop_duplicates(subset=[\"SeriesInstanceUID\"], keep=\"last\").reset_index()\n",
    "df_grouped.index += 1\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for all NM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_nm = []\n",
    "while len(df_info_nm) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesDate = '20191021'\n",
    "    ds.SeriesInstanceUID = list(set(df_nm['Series Instance UID'][0:5]))\n",
    "    ds.PatientID =  list(set(df_nm['Patient ID'][0:5]))\n",
    "    ds.Modality = 'NM'\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info_nm = get_data(config, [ds], to_fetch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Merge the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_series.merge(df_info.drop(columns='Modality').rename(columns={'SeriesInstanceUID': 'Series Instance UID'}), on='Series Instance UID', how='outer')\n",
    "#df_merged[['Series Date', 'Series Time', 'AcquisitionTime']]\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = []\n",
    "while len(df_info) == 0:\n",
    "    ds = Dataset()\n",
    "    ds.QueryRetrieveLevel = 'IMAGE'\n",
    "    ds.SeriesInstanceUID = df_series['Series Instance UID']\n",
    "    ds.PatientID =  df_series['Patient ID']\n",
    "    ds.Modality = 'NM'\n",
    "\n",
    "    # fields to fetch from the DICOM header\n",
    "    to_fetch_fields = ['SeriesInstanceUID', 'PatientID', 'InstanceNumber', 'ManufacturerModelName', 'AcquisitionTime',\n",
    "        'Modality', 'ImageType', 'ActualFrameDuration', 'NumberOfFrames', '0x00540032', '0x00540052']\n",
    "\n",
    "    # find information about this series by fetching some images\n",
    "    df_info = get_data(config, [ds], to_fetch_fields)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
