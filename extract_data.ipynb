{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import holidays\n",
    "from copy import deepcopy\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from scripts.main import *\n",
    "from scripts.retrieve_data import *\n",
    "from scripts.extract_data import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set the width of the notebook\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the config and get the series using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-07'\n",
    "config['main']['end_date'] = '2019-01-08'\n",
    "df_studies = load_transform_and_save_data_from_files(config)\n",
    "display(df_studies.groupby('Date')['Patient ID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the loading from a main studies.pkl file and expanding it, instead of recreating the file every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) get the already processed studies if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-01'\n",
    "config['main']['end_date'] = '2019-01-15'\n",
    "\n",
    "start_date, end_date, days_range = scripts.main.get_day_range(config)\n",
    "studies_save_path = 'data/studies.pkl'\n",
    "df_studies = None\n",
    "already_processed_days = []\n",
    "holiday_days = holidays.Switzerland(prov='VD')\n",
    "\n",
    "# check if the data has already been extracted, transformed and saved\n",
    "if os.path.isfile(studies_save_path):\n",
    "    logging.info('Reading studies database: save file found at \"{}\", loading data'.format(studies_save_path))\n",
    "    df_studies = pd.read_pickle(studies_save_path)\n",
    "    already_processed_days = set(df_studies['Date'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) check whether all required days have already been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_process = [day for day in days_range if day.strftime('%Y%m%d') not in already_processed_days and day.weekday() not in [5, 6] and day not in holiday_days]\n",
    "days_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) process the days that are not present in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in days_to_process:\n",
    "    logging.info('Processing {}: day is required but not present in the main studies DataFrame'.format(day.strftime('%Y%m%d'))\n",
    "\n",
    "    # create a local config object just to process the specified days\n",
    "    local_config = deepcopy(config)\n",
    "    local_config['main']['start_date'] = day.strftime('%Y-%m-%d')\n",
    "    local_config['main']['end_date'] = day.strftime('%Y-%m-%d')\n",
    "                 \n",
    "    # load in the data\n",
    "    df_series = load_data_from_files(local_config)\n",
    "    # mark the rektakes and the machine group for each series\n",
    "    df_series = mark_retakes(local_config, df_series)\n",
    "    df_series = mark_machine_group(local_config, df_series)\n",
    "    # show some info about the series and studies\n",
    "    #show_series_groupby(config, df_series)\n",
    "\n",
    "    # group the series together into a DataFrame of studies\n",
    "    df_studies_for_day = df_series.replace(np.nan, '').groupby('SUID').agg({\n",
    "        'Series Date': lambda x: '/'.join(set(x)),\n",
    "        'Start Time': 'min',\n",
    "        'End Time': 'max',\n",
    "        'Study Description': lambda x: '/'.join(set(x)),\n",
    "        'Patient ID': lambda x: '/'.join(set(x)),\n",
    "        'Machine Group': lambda x: '/'.join(set(x)),\n",
    "        'Modality': lambda x: '/'.join(set(x)),\n",
    "        'Protocol Name': lambda x: '/'.join(set(x))\n",
    "    }).sort_values(['Series Date', 'Start Time', 'Machine Group', 'SUID'])\\\n",
    "    .rename(columns={'Series Date': 'Date'})\n",
    "             \n",
    "    # merge back into the main DataFrame\n",
    "    if df_studies is None:\n",
    "        df_studies = df_studies_for_day\n",
    "    else:\n",
    "        df_studies = pd.concat([df_studies, df_studies_for_day]).sort_values(['Series Date', 'Start Time', 'Machine Group', 'SUID'])\n",
    "\n",
    "# save the newly extented studies to the studies file\n",
    "if len(days_to_process) > 0\n",
    "    df_studies.to_pickle(studies_save_path)\n",
    "\n",
    "df_studies_query = df_studies.query('Date >= \"{}\" & Date <= \"{}\"'.format(start_date, end_date)).copy()\n",
    "return df_studies_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the config and get the series manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-07'\n",
    "config['main']['end_date'] = '2019-04-26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-01-07'\n",
    "config['main']['end_date'] = '2019-04-26'\n",
    "df_series = load_data_from_files(config)\n",
    "\n",
    "indices_to_exclude = []\n",
    "logging.info('Found {} series before filtering description'.format(len(df_series)))\n",
    "for descr_pattern in config['retrieve']['series_descr_patterns_to_exclude'].split('\\n'):\n",
    "    to_exclude_rows = df_series['Series Description'].str.match(descr_pattern, case=False)\n",
    "    # gather all the indices\n",
    "    indices_to_exclude.append(to_exclude_rows[to_exclude_rows == True].index)\n",
    "# flatten the list\n",
    "indices_to_exclude = [index for indices in indices_to_exclude for index in indices.values]\n",
    "# if there is something to exclude, show a message and drop the rows\n",
    "if len(indices_to_exclude) > 0:\n",
    "    logging.info('Found {} series to exclude based on their description: \"{}\"'.format(len(indices_to_exclude),\n",
    "        '\", \"'.join(df_series.loc[indices_to_exclude]['Series Description'])))\n",
    "    df_series.drop(indices_to_exclude, inplace=True)\n",
    "logging.info('Found {} series after filtering description'.format(len(df_series)))\n",
    "\n",
    "# further filter out some Series that are not primary acquisitions (and do not contain any relevant time information)\n",
    "df_series = df_series[~df_series['Protocol Name'].isin(config['retrieve']['series_protocols_to_exclude'].split('\\n'))]\n",
    "logging.debug('Found {} series after filtering protocol names'.format(len(df_series)))\n",
    "\n",
    "#df_series = df_series[~df_series['Series Description'].isin(['Protocole patient', 'Enhancement curve'])]\n",
    "#df_series = df_series[df_series.Machine != 'syngo.via.VB30A']\n",
    "df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series[df_series.Machine == 'syngo.via.VB30A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the marking of the re-takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_series = mark_retakes(config, df_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series, df_count_series, df_count_studies, df_count_series_day, df_count_study_day, df_count_study_weekday = do_series_groupby(config, df_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check what happened in the \"mixed cases\" studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 20): display(df_series[df_series['Machine Group'] == 'mixed cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_series[df_series['Machine Group'] == 'mixed cases']['Study Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some counting on different fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for field in ['Institution Name', 'Machine', 'Machine Group', 'Modality', 'Series Description', 'Study Description', 'Patient ID', 'i_take']:\n",
    "    logging.info('Number of *Series* groupped by \"{}\"'.format(field))\n",
    "    display(df_series.groupby(field)['SUID'].count())\n",
    "    logging.info('Number of *Studies* groupped by \"{}\"'.format(field))\n",
    "    display(df_series.groupby([field, 'SUID']).count().reset_index().groupby(field)['SUID'].count())\n",
    "    logging.info('='*160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out the start and end times of each study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies = df_series.dropna().groupby('SUID').agg({\n",
    "    'Series Date': lambda x: '/'.join(set(x)),\n",
    "    'Start Time': 'min',\n",
    "    'End Time': 'max',\n",
    "    'Study Description': lambda x: '/'.join(set(x)),\n",
    "    'Machine Group': lambda x: '/'.join(set(x)),\n",
    "    'Modality': lambda x: '/'.join(set(x)),\n",
    "    'Institution Name': lambda x: '/'.join(set(x)),\n",
    "    'Protocol Name': lambda x: '/'.join(set(x))\n",
    "}).sort_values(['Series Date', 'Machine Group', 'Start Time', 'SUID'])\n",
    "studies_save_path = 'data/studies/studies_{}_{}.pkl'.format(config['main']['start_date'], config['main']['end_date']).replace('-', '')\n",
    "df_studies.to_pickle(studies_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies = load_transform_and_save_data_from_files(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the series descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_studies['Study Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
