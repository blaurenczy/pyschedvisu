{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import os\n",
    "os.chdir('H:/Mes Documents/ServiceCivil2019/schedvisu')\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from main import *\n",
    "from retrieve_data import *\n",
    "from extract_data import *\n",
    "from create_report import *\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set the width of the notebook\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUGGING mark retakes overlap errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '20190226'\n",
    "config['main']['end_date'] = '20190226'\n",
    "#patientIDs = ['2254895']\n",
    "#config['retrieve']['debug_patient_ids'] = ','.join(patientIDs)\n",
    "\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUGGING mark retakes and FDG cerveau error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "df_studies, _ = load_transform_and_save_data_from_files(config)\n",
    "create_report(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FDG cerveau end times are not properly calculated, ContentTime is also not the good DICOM field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = pd.read_pickle('C:/TEMP/SchedVisu_data/2019/2019-10/20191002.pkl')\n",
    "df_fdg_cerveau = df_series[df_series['Series Description'].str.match('.*FDG Cerveau.*')]\n",
    "start_times = pd.to_datetime(df_fdg_cerveau['Start Time'], format='%H%M%S')\n",
    "duration_seconds = [int(60 * minutes[0]) for minutes in df_fdg_cerveau['Series Description'].str.extract('.*FDG Cerveau (.+)min.*').astype(int).values]\n",
    "end_times = [(start_times.iloc[i] + timedelta(seconds=duration_seconds[i])).strftime('%H%M%S') for i in range(len(start_times))]\n",
    "df_series.loc[df_fdg_cerveau.index, 'End Time'] = end_times\n",
    "df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUGGING FDG cerveau mark retakes error, and NM series duration not found error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "#import shutil\n",
    "#if os.path.exists(config['retrieve']['data_dir']): shutil.rmtree(config['retrieve']['data_dir'])\n",
    "\n",
    "config['main']['start_date'] = '20190107'\n",
    "config['main']['end_date'] = '20190108'\n",
    "#patientIDs = ['2254895']\n",
    "#config['retrieve']['debug_patient_ids'] = ','.join(patientIDs)\n",
    "\n",
    "retrieve_and_save_data_from_PACS(config)\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)\n",
    "\n",
    "create_report(config)\n",
    "display(df_studies)\n",
    "#display(df_series[['Patient ID', 'Date', 'Series Time', 'Start Time', 'End Time', 'Modality', 'Machine', 'Series Description']])\n",
    "logging.info(df_series.columns)\n",
    "#display(df_series[['Patient ID', 'Series Description', 'ActualFrameDuration',\n",
    "#       'Start Time', 'End Time', 'ContentTime_end', 'ContentTime_start', 'Number of Series Related Instances', 'Protocol Name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finding a fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_ctpt_clean = pd.read_pickle('df_info_ctpt_clean.pkl')\n",
    "df_info_nm_clean = pd.read_pickle('df_info_nm_clean.pkl')\n",
    "\n",
    "display(df_info_ctpt_clean)\n",
    "df_info_ctpt_clean['ImageType'].apply(str).str.match('.*SECONDARY.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_series[(df_series['CTDIvol'].isnull()) & (df_series['Modality'] == 'CT')]['Machine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_series[(df_series['CTDIvol'].isnull()) & (df_series['Modality'] == 'CT')]['Protocol Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_series[(~df_series['CTDIvol'].isnull()) & (df_series['Modality'] == 'CT')]['Machine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_series[(~df_series['CTDIvol'].isnull()) & (df_series['Modality'] == 'CT')]['Protocol Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUGGING FDG Cerveau PET series&studies have wrong end time #18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "day = dt(2019, 1, 9)\n",
    "day_str = day.strftime('%Y%m%d')\n",
    "config['main']['start_date'] = day_str\n",
    "config['main']['end_date'] = day_str\n",
    "patientIDs = ['3232471']\n",
    "config['retrieve']['debug_patient_ids'] = ','.join(patientIDs)\n",
    "\n",
    "retrieve_and_save_data_from_PACS(config)\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)\n",
    "\n",
    "#display(df_studies)\n",
    "#display(df_series[['Patient ID', 'Series Time', 'Start Time', 'End Time', 'Modality', 'Machine', 'Series Description']])\n",
    "#logging.info(df_series.columns)\n",
    "display(df_series[['Patient ID', 'Series Description', 'ActualFrameDuration',\n",
    "       'Start Time', 'End Time', 'ContentTime_end', 'ContentTime_start', 'Number of Series Related Instances', 'Protocol Name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finding a fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_end_times = df_series[pd.to_datetime(df_series['ContentTime_end'], format='%H%M%S') > pd.to_datetime(df_series['End Time'], format='%H%M%S')]\n",
    "df_series.loc[wrong_end_times.index, 'End Time'] = wrong_end_times['ContentTime_end']\n",
    "df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUGGING \"Some studies are overlapping, although they should be split up\" #19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare the studies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "day = dt(2019, 2, 27)\n",
    "day_str = day.strftime('%Y%m%d')\n",
    "config['main']['start_date'] = day_str\n",
    "config['main']['end_date'] = day_str\n",
    "#patientIDs = ['2026682', '138821']\n",
    "#config['retrieve']['debug_patient_ids'] = ','.join(patientIDs)\n",
    "\n",
    "retrieve_and_save_data_from_PACS(config)\n",
    "df_studies, df_series = load_transform_and_save_data_from_files(config)\n",
    "\n",
    "#display(df_studies)\n",
    "#display(df_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Fetch info for series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series, df_series_failed = fetch_info_for_series_with_batches(config, df_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mark retakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the list of Study Instance UIDs\n",
    "study_UIDs = list(set(df_series['Study Instance UID']))\n",
    "logging.info('Found {} unique study UIDs'.format(len(study_UIDs)))\n",
    "\n",
    "FMT = '%H%M%S'\n",
    "# get from the config the threshold in seconds for splitting a study in \"first/second take\"\n",
    "study_split_thresh = int(config['extract']['n_sec_second_take_split_thresh'])\n",
    "\n",
    "# create a column to mark the \"take\" index of the series. By default, everything is a first take\n",
    "df_series['i_take'] = None\n",
    "\n",
    "# build the list of rows to keep\n",
    "indices_to_keep = []\n",
    "for i_study in range(len(study_UIDs)):\n",
    "\n",
    "    # get the series related to the current Study Instance UID and sort them\n",
    "    sUID = study_UIDs[i_study]\n",
    "    df_series_for_study = df_series[df_series['Study Instance UID'] == sUID].sort_values('Start Time')\n",
    "\n",
    "    study_str = '[{:4d}/{:4d}] IPP {}, {:52}'.format(i_study, len(study_UIDs) - 1, df_series_for_study.iloc[0]['Patient ID'], sUID)\n",
    "    logging.debug('Processing  {}: found {:2d} series'.format(study_str, len(df_series_for_study)))\n",
    "\n",
    "    # there must be at least 2 series for any splitting\n",
    "    if len(df_series_for_study) < 2:\n",
    "        df_series.loc[df_series_for_study.index, 'i_take'] = 1\n",
    "        continue\n",
    "\n",
    "    # convert the columns to datetime format\n",
    "    df_series_for_study['Start Time'] = pd.to_datetime(df_series_for_study['Start Time'], format=FMT)\n",
    "    df_series_for_study['End Time'] = pd.to_datetime(df_series_for_study['End Time'], format=FMT)\n",
    "    # compare the start time of a row with the end time of the previous row\n",
    "    df_series_for_study['time_to_prev'] = df_series_for_study['End Time'].shift() - df_series_for_study['Start Time']\n",
    "    # correct for negative durations (when start time is before end time of previous row)\n",
    "    df_series_for_study.loc[df_series_for_study['time_to_prev'] < timedelta(0), 'time_to_prev'] *= -1\n",
    "    # get the series where a split should be done\n",
    "    df_series_split = df_series_for_study[df_series_for_study['time_to_prev'] > timedelta(seconds=study_split_thresh)]\n",
    "    \n",
    "    # also check whether there is a series from another study inbetween our study\n",
    "    df_series_other = df_series[(df_series['Study Instance UID'] != sUID) & (df_series['Machine Group'] == df_series_for_study.iloc[0]['Machine Group'])]\n",
    "    start_times_other = pd.to_datetime(df_series_other['Start Time'], format=FMT)\n",
    "    end_times_other = pd.to_datetime(df_series_other['End Time'], format=FMT)\n",
    "    study_start = min(df_series_for_study['Start Time'])\n",
    "    study_end = max(df_series_for_study['End Time'])    \n",
    "    df_series_other_inbetween = df_series_other[(start_times_other > study_start) & (end_times_other < study_end)]\n",
    "    n_series_inbetween = len(df_series_other_inbetween)\n",
    "    if n_series_inbetween > 0:\n",
    "        inbetween_start = min(pd.to_datetime(df_series_other_inbetween['Start Time'], format=FMT))\n",
    "        new_series_split = df_series_for_study[df_series_for_study['Start Time'] > inbetween_start].sort_values('Start Time').iloc[0]\n",
    "        df_series_split = df_series_split.append(df_series.loc[new_series_split.name])\n",
    "    # if there is no splitting indices\n",
    "    if len(df_series_split) == 0:\n",
    "        logging.debug('  Passing   {}: no second take (max time diff: {})'\n",
    "            .format(study_str, max(df_series_for_study['time_to_prev'])))\n",
    "        df_series.loc[df_series_for_study.index, 'i_take'] = 1\n",
    "        continue\n",
    "\n",
    "    # if there is more than one split point, throw an error and do not do any splitting\n",
    "    elif len(df_series_split) >= 1:\n",
    "        logging.info('  Found {} series to split'.format(len(df_series_split)))\n",
    "        # go through all the series\n",
    "        i_take = 1\n",
    "        for ind in df_series_for_study.index:\n",
    "            if ind in df_series_split.index:\n",
    "                if ind <= 0:\n",
    "                    logging.error('  Error at {}: trying to split at index \"{}\". Aborting.'\n",
    "                        .format(study_str, ind))\n",
    "                    continue\n",
    "                logging.info('  Splitting {}: split {} between {:3d}/{:3d} [T={}/{}, D={}]'\n",
    "                    .format(study_str, i_take, ind - 1, ind, df_series.loc[ind - 1, 'End Time'],\n",
    "                    df_series.loc[ind, 'Start Time'], df_series_for_study.loc[ind, 'time_to_prev']))\n",
    "                i_take += 1\n",
    "            # mark the series according to the split index\n",
    "            df_series.loc[ind, 'i_take'] = i_take\n",
    "\n",
    "# create a new unique ID that includes the retake information\n",
    "df_series['SUID'] = df_series['Study Instance UID'] + '_' + df_series['i_take'].astype(str)\n",
    "\n",
    "display(df_series.iloc[:, [0,1,2,3,4,5,6,7,24,25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Read in all the data files day by day and search for problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config['main']['start_date'] = '2019-12-02'\n",
    "config['main']['end_date'] = '2019-12-02'\n",
    "\n",
    "# get the date range from the config\n",
    "start_date = dt.strptime(config['main']['start_date'], '%Y-%m-%d')\n",
    "end_date = dt.strptime(config['main']['end_date'], '%Y-%m-%d')\n",
    "days_range = pd.date_range(start_date, end_date)\n",
    "\n",
    "# create the variable holding all the series for all days\n",
    "df_all_problems = None\n",
    "df_all_series = None\n",
    "\n",
    "# go through the date range day by day\n",
    "for day in days_range:\n",
    "\n",
    "    config['main']['start_date'] = day.strftime('%Y-%m-%d')\n",
    "    config['main']['end_date'] = day.strftime('%Y-%m-%d')\n",
    "    \n",
    "    df = load_data_from_files(config)\n",
    "    if df is None: continue\n",
    "    df_problem = df[\n",
    "            (df['Start Time'].isnull())\n",
    "            | (df['End Time'].isnull())\n",
    "            | (df['Machine'] == '')\n",
    "            | (df['Machine'].isnull())\n",
    "            | (df['Institution Name'] == '')\n",
    "            | (df['Institution Name'] == 'NONE')\n",
    "            | (df['Institution Name'].isnull())]\n",
    "    df_all_series = pd.concat([df_all_series, df], sort=False)\n",
    "    if len(df_problem) <= 0: continue\n",
    "    df_all_problems = pd.concat([df_all_problems, df_problem], sort=False)\n",
    "    logging.info('Current day: {}'.format(day.strftime('%Y-%m-%d')))\n",
    "    with pd.option_context(\"display.max_colwidth\", 1000): display(df_problem)\n",
    "    with pd.option_context(\"display.max_colwidth\", 1000): display(df[df['Series Instance UID'].isin(df_problem['Series Instance UID'])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
